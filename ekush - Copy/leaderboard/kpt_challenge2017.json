[{"description": "Our keypoint results were obtained by single-person skeleton models, on the human bounding boxes generated by MegDet, a large-batch object detector by Megvii (Face++). The single-person skeleton model is based on a ResNet with U-shape side branch to integrate high resolution information from the earlier layers and semantic information from the deeper layers. Large kernel [1] convolution was utilized to enlarge the effective receptive field. Ensemble was built on the feature-map level to generate the final results. Our method was trained on the provided labeled images only.", "leaderboard_id": "15", "url": "https://arxiv.org/abs/1711.07319", "results": "{\"AP_50\": 0.905, \"AR_medium\": 0.743, \"AR_50\": 0.947, \"AP_large\": 0.781, \"AR_75\": 0.848, \"AP\": 0.721, \"AP_medium\": 0.679, \"AR\": 0.787, \"AR_large\": 0.847, \"AP_75\": 0.789}", "results_details": {"ensemble": true, "external_data": false}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1244, "members": "Yilun Chen*, Zhicheng Wang*, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Jian Sun (* indicates equal contribution); Megvii Research", "name": "Megvii (Face++)"}, "date": "2017-10-29", "publication": "https://arxiv.org/abs/1703.02719"}, {"description": "We implement a two stage top-down method. In the first stage, we use mask rcnn like framework to detect human bounding boxes under both supervision from detection and keypoints. In the second stage, we perform single person pose estimation. In addition, we remove false annotated data based on the keypoint annotation and detection annotation jointly. We also use both detection and keypoint score for instance re-scoring and perform box-nms and oks-nms based on it. In both stages we use only single model, no model ensembles are applied. Additional data are from a public dataset of AI-Challenger keypoints detection competition track and provide 1.5 point AP improvement on the validation dataset.", "leaderboard_id": "15", "url": "https://github.com/MVIG-SJTU/RMPE", "results": "{\"AP_50\": 0.894, \"AR_medium\": 0.718, \"AR_50\": 0.936, \"AP_large\": 0.791, \"AR_75\": 0.834, \"AP\": 0.714, \"AP_medium\": 0.659, \"AR\": 0.772, \"AR_large\": 0.845, \"AP_75\": 0.781}", "results_details": {"ensemble": false, "external_data": true}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1245, "members": "Changbao Wang (Beihang University), Yujie Wang (Beihang University), Quanquan Li (SenseTime Group Limited)", "name": "oks"}, "date": "2017-10-29", "publication": ""}, {"description": "We build a two-stage system consisting of a bounding-box person detector followed by a human pose estimator. For person detection, we train a faster rcnn with deformable version of resnet101 on MS-COCO trainval2017. Multi-scale testing and soft-NMS are used. For pose estimation, we train a resnet-101 based model to predict heatmap and offset fields of keypoints for the person in the detected bounding box. The performance is boosted by OKS-based NMS in post-processing. The pose estimator is trained using COCO MS-trainval2017 and extra dataset. For single model, we obtain an average precision of 0.713 on the test-dev set and 0.688 on the test-challenge set. Finally, we use an ensemble of seven models and obtain an average precision of 0.728 on the test-dev set and 0.706 on the test-challenge set.", "leaderboard_id": "15", "url": "", "results": "{\"AP_50\": 0.880, \"AR_medium\": 0.718, \"AR_50\": 0.936, \"AP_large\": 0.792, \"AR_75\": 0.830, \"AP\": 0.706, \"AP_medium\": 0.656, \"AR\": 0.774, \"AR_large\": 0.850, \"AP_75\": 0.765}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1246, "members": "Xinze Chen*, Rui Wu*, Yun Ye and Guan Huang (*The first two authors contribute equally to this work)", "name": "bangbangren"}, "date": "2017-10-29", "publication": ""}, {"description": "Our method is based on the two-stage approach described in our CVPR 2017 paper (https://arxiv.org/abs/1701.01779), with some improvements (use of a Resnet-152 instead of a ResNet-101 backbone, better feature alignment, more refined training schedule, etc). Our final entry in the competition uses a single model (no ensembling). In addition to the COCO images and annotations (and Imagenet image classification pretraining), we also used for training our model an internal dataset of images with keypoint annotations which is described in detail in Sec. 4.1 of our paper (same number of extra annotations as our last year's entry to the competition). These extra data improved keypoints test-dev AP by 3-4%.", "leaderboard_id": "15", "url": "", "results": "{\"AP_50\": 0.859, \"AR_medium\": 0.697, \"AR_50\": 0.907, \"AP_large\": 0.745, \"AR_75\": 0.807, \"AP\": 0.691, \"AP_medium\": 0.660, \"AR\": 0.751, \"AR_large\": 0.824, \"AP_75\": 0.752}", "results_details": {"ensemble": false, "external_data": true}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1247, "members": "George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, Hartwig Adam, Kevin Murphy (Google Research)", "name": "G-RMI"}, "date": "2017-10-29", "publication": "https://arxiv.org/abs/1701.01779"}, {"description": "Our keypoint entry is build upon the several papers developed at FAIR: (i) Mask R-CNN [1], for bbox and keypoint prediction; (ii) Feature Pyramid Networks (FPN) [2], and (iii) ResNeXt [3] 101-32x8d pre-trained on ImageNet-5k as in [3], as the backbone. In this entry, we highlight our newly developed large-scale semi-supervised learning paradigm which we call Data Distillation [4, to appear]. We apply Mask R-CNN with test-time augmentation on a large set of unlabeled images (Sports-1M frames without using any annotations), and we view these predictions as new annotations which we combine with the COCO images to retrain our model. No data labeled with human boxes, masks, or keypoints, beyond what is provided in COCO, are used in our entry. Main results on test-dev include: (i) Single-model, Mask R-CNN baseline, with FPN and ResNeXt-101-32x8d: 65.8 keypoint AP (ii) Single-model, plus Data Distillation from unlabeled Sports-1M: 67.0 keypoint AP  (iii) Single-model, Data Distillation plus test time augmentations (horizontal flip, multi-scale): 69.2 keypoint AP [1] https://arxiv.org/abs/1703.06870 [2] https://arxiv.org/abs/1612.03144 [3] https://arxiv.org/abs/1611.05431 [4] in preparation.", "leaderboard_id": "15", "url": "", "results": "{\"AP_50\": 0.892, \"AR_medium\": 0.702, \"AR_50\": 0.932, \"AP_large\": 0.768, \"AR_75\": 0.812, \"AP\": 0.689, \"AP_medium\": 0.637, \"AR\": 0.754, \"AR_large\": 0.826, \"AP_75\": 0.752}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1248, "members": "*Georgia Gkioxari, *Ilija Radosavovic, Kaiming He, Ross Girshick, Piotr Doll√°r (* denotes equal co-first authorship); Facebook AI Research", "name": "FAIR Mask R-CNN"}, "date": "2017-10-29", "publication": "https://arxiv.org/abs/1703.06870"}, {"description": "We propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections. We train our single person pose estimator on both COCO and AI challenger dataset. Our single model performance achieved 68.5 mAP on test-dev set. We were able to achieve 69.7 mAP on test-dev set by fusing 3 models after the competition. The inference time for an image is around 1-2s.", "leaderboard_id": "15", "url": "", "results": "{\"AP_50\": 0.867, \"AR_medium\": 0.686, \"AR_50\": 0.908, \"AP_large\": 0.750, \"AR_75\": 0.795, \"AP\": 0.680, \"AP_medium\": 0.633, \"AR\": 0.735, \"AR_large\": 0.804, \"AP_75\": 0.747}", "results_details": {"ensemble": true, "external_data": true}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1249, "members": "Hao-Shu Fang, Shanghai Jiao Tong University; Jiefeng Li, Shanghai Jiao Tong University; Ruiheng Chang, Shanghai Jiao Tong University; Jinkun Cao, Shanghai Jiao Tong University; Yinghong Fang, Shanghai Jiao Tong University; Yu-Wing Tai, Tencent YouTu; Cewu Lu(*corresponding author), Shanghai Jiao Tong University", "name": "SJTU"}, "date": "2017-10-29", "publication": ""}, {"description": "The submitted results are obtained by using the 'divide and combine' technique and AE method [1]. [1]Associative Embedding: End-to-End Learning for Joint Detection and Grouping Newell et al. NIPS 2017.", "leaderboard_id": "15", "url": "", "results": "{\"AP_50\": 0.843, \"AR_medium\": 0.631, \"AR_50\": 0.881, \"AP_large\": 0.718, \"AR_75\": 0.745, \"AP\": 0.636, \"AP_medium\": 0.576, \"AR\": 0.694, \"AR_large\": 0.782, \"AP_75\": 0.691}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1250, "members": "Yue Liao, Yao Sun, Si Liu, Yinglu Liu, Yanli Li, Junjun Xiong", "name": "iie-samsung-pose"}, "date": "2017-10-29", "publication": ""}, {"description": "METU is a pipeline approach that combines a person detector (PD) with a single person pose estimator. The erroneous detection from PD was corrected via adaptive thresholding of confidence results.", "leaderboard_id": "15", "url": "", "results": "{\"AP_50\": 0.686, \"AR_medium\": 0.427, \"AR_50\": 0.721, \"AP_large\": 0.563, \"AR_75\": 0.589, \"AP\": 0.448, \"AP_medium\": 0.371, \"AR\": 0.522, \"AR_large\": 0.650, \"AP_75\": 0.503}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1251, "members": "M. Salih Karagoz, Muhammed Kocabas.", "name": "METU"}, "date": "2017-10-29", "publication": ""}, {"description": "", "leaderboard_id": "15", "url": "", "results": "{\"AP_50\": 0.713, \"AR_medium\": 0.475, \"AR_50\": 0.782, \"AP_large\": 0.538, \"AR_75\": 0.610, \"AP\": 0.446, \"AP_medium\": 0.382, \"AR\": 0.560, \"AR_large\": 0.676, \"AP_75\": 0.477}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1252, "members": "", "name": "Jessie33321"}, "date": "2017-10-29", "publication": ""}, {"description": "", "leaderboard_id": "15", "url": "", "results": "{\"AP_50\": 0.642, \"AR_medium\": 0.548, \"AR_50\": 0.826, \"AP_large\": 0.340, \"AR_75\": 0.649, \"AP\": 0.389, \"AP_medium\": 0.498, \"AR\": 0.609, \"AR_large\": 0.692, \"AP_75\": 0.394}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1253, "members": "", "name": "Gnxr9"}, "date": "2017-10-29", "publication": ""}, {"description": "", "leaderboard_id": "15", "url": "", "results": "{\"AP_50\": 0.519, \"AR_medium\": 0.406, \"AR_50\": 0.759, \"AP_large\": 0.272, \"AR_75\": 0.532, \"AP\": 0.280, \"AP_medium\": 0.348, \"AR\": 0.515, \"AR_large\": 0.662, \"AP_75\": 0.261}", "results_details": {"ensemble": false, "external_data": false}, "leaderboard_name": "kpt-challenge2017", "team": {"id": 1253, "members": "", "name": "LwhL"}, "date": "2017-10-29", "publication": ""}]
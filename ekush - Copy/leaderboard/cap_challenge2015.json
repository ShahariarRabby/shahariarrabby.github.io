[{"leaderboard_id": 3, "date": "2015-05-29", "description": "This is the Google CNN + an LSTM from the CVPR 2015 paper.", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1411.4555", "team": {"id": 41, "members": "", "name": "Google"}, "results": "{\"q1\": 0.273, \"q3\": 4.107, \"q2\": 0.317, \"q5\": 0.233, \"q4\": 2.742}", "results_details": null, "publication": "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, CVPR 2015"}, {"leaderboard_id": 3, "date": "2015-04-23", "description": "JK from 301", "leaderboard_name": "cap-challenge2015", "url": "", "team": {"id": 42, "members": "", "name": "Tsinghua Bigeye"}, "results": "{\"q1\": 0.1, \"q3\": 3.51, \"q2\": 0.146, \"q5\": 0.116, \"q4\": 2.163}", "results_details": null, "publication": ""}, {"leaderboard_id": 3, "date": "2015-04-08", "description": "Initial output with VGG network", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1411.4952", "team": {"id": 43, "members": "", "name": "MSR"}, "results": "{\"q1\": 0.268, \"q3\": 4.137, \"q2\": 0.322, \"q5\": 0.234, \"q4\": 2.662}", "results_details": null, "publication": "H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Doll\u00e1r, J. Gao, X. He, M. Mitchell, J. Platt, C.L. Zitnick, and G. Zweig, From Captions to Visual Concepts and Back, CVPR 2015"}, {"leaderboard_id": 3, "date": "2015-03-23", "description": "Human baseline", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/pdf/1504.00325.pdf", "team": {"id": 44, "members": "", "name": "Human"}, "results": "{\"q1\": 0.638, \"q3\": 4.836, \"q2\": 0.675, \"q5\": 0.352, \"q4\": 3.428}", "results_details": null, "publication": "Human Baseline"}, {"leaderboard_id": 3, "date": "2015-05-26", "description": "Combination of multiple neuraltalk based models using mutual evaluation.", "leaderboard_name": "cap-challenge2015", "url": "", "team": {"id": 45, "members": "", "name": "PicSOM"}, "results": "{\"q1\": 0.202, \"q3\": 3.965, \"q2\": 0.25, \"q5\": 0.182, \"q4\": 2.552}", "results_details": null, "publication": ""}, {"leaderboard_id": 3, "date": "2015-05-29", "description": "We train linear classifiers for bigrams that commonly occur in train dataset using PAAPL (Ushiku et al. : Efficient image annotation for automatic sentence generation).", "leaderboard_name": "cap-challenge2015", "url": "", "team": {"id": 46, "members": "", "name": "MIL"}, "results": "{\"q1\": 0.168, \"q3\": 3.349, \"q2\": 0.197, \"q5\": 0.159, \"q4\": 2.915}", "results_details": null, "publication": ""}, {"leaderboard_id": 3, "date": "2015-04-10", "description": "Our submission is an ensemble of 4 multimodal log-bilinear models [1,2] whose hyperparameters are tuned using Bayesian optimization with DNGO (Deep networks for global optimization) [3]. A beam search is used for generation.", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/pdf/1411.2539.pdf", "team": {"id": 47, "members": "", "name": "MLBL"}, "results": "{\"q1\": 0.167, \"q3\": 3.659, \"q2\": 0.196, \"q5\": 0.156, \"q4\": 2.42}", "results_details": null, "publication": "Multimodal Neural Language Models (Kiros et al, ICML 2014), Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models (Kiros et al, arXiv 2014), and Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al, arXiv 2015)"}, {"leaderboard_id": 3, "date": "2015-05-15", "description": "k-Nearest Neighbor approach.", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1505.04467", "team": {"id": 48, "members": "", "name": "Nearest Neighbor"}, "results": "{\"q1\": 0.216, \"q3\": 3.801, \"q2\": 0.255, \"q5\": 0.196, \"q4\": 2.716}", "results_details": null, "publication": "Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv 2015"}, {"leaderboard_id": 3, "date": "2015-05-28", "description": "Maximum entropy language model + DMSM + gated recurrent neural network + part-of-speech features + cider tuning", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1505.01809", "team": {"id": 49, "members": "", "name": "MSR Captivator"}, "results": "{\"q1\": 0.25, \"q3\": 4.149, \"q2\": 0.301, \"q5\": 0.233, \"q4\": 2.565}", "results_details": null, "publication": "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv 2015"}, {"leaderboard_id": 3, "date": "2015-05-30", "description": "This is an updated version of the m-RNN model. ", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1504.06692", "team": {"id": 50, "members": "", "name": "m-RNN"}, "results": "{\"q1\": 0.223, \"q3\": 3.897, \"q2\": 0.252, \"q5\": 0.202, \"q4\": 2.595}", "results_details": null, "publication": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images. arXiv preprint arXiv:1504.06692 (2015) "}, {"leaderboard_id": 3, "date": "2015-05-14", "description": "An ensemble of 7 soft attention models.", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1502.03044 ", "team": {"id": 51, "members": "", "name": "Montreal/Toronto"}, "results": "{\"q1\": 0.262, \"q3\": 3.932, \"q2\": 0.272, \"q5\": 0.197, \"q4\": 2.832}", "results_details": null, "publication": "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, arXiv 2015"}, {"leaderboard_id": 3, "date": "2015-04-15", "description": "One layer Vanilla LSTM with 512 hidden units, 512-D words vectors trained from scratch. Beam size 1. Trained with RMSProp, gradient clipping.", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1412.2306", "team": {"id": 52, "members": "", "name": "NeuralTalk"}, "results": "{\"q1\": 0.166, \"q3\": 3.436, \"q2\": 0.192, \"q5\": 0.147, \"q4\": 2.742}", "results_details": null, "publication": "Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Descriptions, CVPR 2015"}, {"leaderboard_id": 3, "date": "2015-05-29", "description": "disclosed in future publication", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1506.03995 ", "team": {"id": 53, "members": "", "name": "Brno University"}, "results": "{\"q1\": 0.194, \"q3\": 3.079, \"q2\": 0.213, \"q5\": 0.154, \"q4\": 3.482}", "results_details": null, "publication": "Martin Kolar, Michal Hradis, Pavel Zemcik, Technical Report: Image Captioning with Semantically Similar Images, arXiv 2015"}, {"leaderboard_id": 3, "date": "2015-05-26", "description": "This is an updated version of the m-RNN model. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. We only use MS COCO data. More details can be found at http://www.stat.ucla.edu/~junhua.mao/m-RNN.html", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1412.6632", "team": {"id": 54, "members": "", "name": "m-RNN (Baidu/ UCLA)"}, "results": "{\"q1\": 0.19, \"q3\": 3.831, \"q2\": 0.241, \"q5\": 0.195, \"q4\": 2.548}", "results_details": null, "publication": "Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan, Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN), arXiv 2014"}, {"leaderboard_id": 3, "date": "2015-04-25", "description": "[See LRCN paper for full description.] This is the &amp;amp;amp;quot;two layer, factored&amp;amp;amp;quot; variant of the LRCN architecture.  We used VGGNet as the base visual network.  Sentences are generated using beam search with beam size 1.", "leaderboard_name": "cap-challenge2015", "url": "http://arxiv.org/abs/1411.4389", "team": {"id": 55, "members": "", "name": "Berkeley LRCN"}, "results": "{\"q1\": 0.246, \"q3\": 3.924, \"q2\": 0.268, \"q5\": 0.204, \"q4\": 2.786}", "results_details": null, "publication": "J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR 2015"}, {"leaderboard_id": 3, "date": "2015-05-26", "description": "Add an intermediate attributes layer between CNN and LSTM", "leaderboard_name": "cap-challenge2015", "url": "", "team": {"id": 58, "members": "", "name": "ACVT"}, "results": "{\"q1\": 0.154, \"q3\": 3.516, \"q2\": 0.19, \"q5\": 0.155, \"q4\": 2.599}", "results_details": null, "publication": ""}, {"leaderboard_id": 3, "date": "2015-05-29", "description": "", "leaderboard_name": "cap-challenge2015", "url": "", "team": {"id": 57, "members": "", "name": "Random"}, "results": "{\"q1\":0.007,\"q2\":0.020,\"q3\":1.084,\"q4\":3.247,\"q5\":0.013}", "results_details": null, "publication": ""}]
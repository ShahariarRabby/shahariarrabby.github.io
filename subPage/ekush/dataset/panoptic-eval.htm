<h1>1. Panoptic Evaluation</h1>
<p>This page describes the <i>panoptic evaluation metrics</i> used by COCO. The evaluation code provided here can be used to obtain results on the publicly available COCO validation set. It computes multiple metrics described below. To obtain results on the COCO test set, for which ground-truth annotations are hidden, generated results must be <a href="#upload">uploaded</a> to the evaluation server. The exact same evaluation code, described below, is used to evaluate results on the test set.</p>
<p><b>Note: the <a href="#download">data</a>, <a href="https://github.com/cocodataset/panopticapi">evaluation code</a>, and <a href="https://competitions.codalab.org/competitions/19507" target="_blank">evaluation server</a> for the panoptic segmentation task are now available! Everything is likely finalized, but if you find any issues with the data, code, or evaluation server please let us know.</b></p>

<h1>2. Classes</h1>
<p>The panoptic task includes all 80 thing categories from the <a href="#detection-2018">detection</a> task and a subset of the 91 stuff categories from the <a href="#stuff-2018">stuff</a> task, with any overlaps manually resolved. Some of the original stuff categories are either <a href="#stuff-merged" data-toggle="collapse" aria-expanded="false" aria-controls="stuff-merged"><i class="glyphicon glyphicon-chevron-right"></i><i class="glyphicon glyphicon-chevron-down"></i>merged or ignored</a> to ensure higher consistency of the ground truth (full details will be given in the paper). Things and stuff categories have the following ranges of ids (note that not all ids are used due to the removal of some classes):</p>
<div class="json fontMono">
  <div class="jsonk">1-91</div><div class="jsonv">% thing categories (80 total)</div>
  <div class="jsonk">92-182</div><div class="jsonv">% original stuff categories (36 total)</div>
  <div class="jsonk">183-200</div><div class="jsonv">% merged stuff categories (17 total)</div>
</div>
<div id="stuff-merged" class="collapse"><br/>
  <div class="json">
    <div class="jsonreg">The following new stuff categories were created by merging old stuff categories:</div>
    <div class="jsontab">
      <div class="jsonreg"><b>tree-merged</b>: branch, tree, bush, leaves</div>
      <div class="jsonreg"><b>fence-merged</b>: cage, fence, railing</div>
      <div class="jsonreg"><b>ceiling-merged</b>: ceiling-tile, ceiling-other</div>
      <div class="jsonreg"><b>sky-other-merged</b>: clouds, sky-other, fog</div>
      <div class="jsonreg"><b>cabinet-merged</b>: cupboard, cabinet</div>
      <div class="jsonreg"><b>table-merged</b>: desk-stuff, table</div>
      <div class="jsonreg"><b>floor-other-merged</b>:  floor-marble, floor-other, floor-tile</div>
      <div class="jsonreg"><b>pavement-merged</b>: floor-stone, pavement</div>
      <div class="jsonreg"><b>mountain-merged</b>: hill, mountain</div>
      <div class="jsonreg"><b>grass-merged</b>: moss, grass, straw</div>
      <div class="jsonreg"><b>dirt-merged</b>: mud, dirt</div>
      <div class="jsonreg"><b>paper-merged</b>: napkin, paper</div>
      <div class="jsonreg"><b>food-other-merged</b>: salad, vegetable, food-other</div>
      <div class="jsonreg"><b>building-other-merged</b>: skyscraper, building-other</div>
      <div class="jsonreg"><b>rock-merged</b>: stone, rock</div>
      <div class="jsonreg"><b>wall-other-merged</b>: wall-other, wall-concrete, wall-panel</div>
      <div class="jsonreg"><b>rug-merged</b>: mat, rug, carpet</b></div>
    </div><br/>
    <div class="jsonreg">The following stuff categories were removed (their pixels are set to void):</div>
    <div class="jsontab">
      <div class="jsonreg">furniture-other, metal, plastic, solid-other, structural-other, waterdrops, textile-other, cloth, clothes, plant-other, wood, ground-other</div>
    </div>
  </div>
</div>

<h1>3. Panoptic Quality Overview</h2>
<p>Panoptic segmentation aims to unify the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). Existing metrics are specialized for either semantic or instance segmentation and cannot be used to evaluate the joint task involving both stuff and thing classes. Rather than using a heuristic combination of disjoint metrics for the two tasks, the panoptic task introduces a new Panoptic Quality (PQ) metric. PQ evaluates performance for all categories, including both stuff and thing categories, in a unified manner.</p>
<p>Computing PQ involves two steps: (1) segment matching and (2) PQ computation given the matches. Both steps are simple and efficient. First, for each image, the ground truth and predicted segments are matched with an IoU threshold 0.5. Since panoptic segmentation requires non-overlapping segments, if a threshold of 0.5 is used, <i>the matching is unique</i> (a simple proof is in the paper). That is, there can be at most one match per segment and so it is trivial to obtain the matches. After matching, each segment falls into one of three sets: TP (matched pairs), FP (unmatched predicted segments), and FN (unmatched ground truth segments). Given the TP, FP, and FN, the Panoptic Quality (PQ) metric for each category is simply:</p>
<div class="json fontMono">
  $$ \text{PQ} = \frac{\sum_{(p, g) \in \mathit{TP}} \text{IoU}(p, g)}{|\mathit{TP}| + \frac{1}{2}|\mathit{FP}| + \frac{1}{2}|\mathit{FN}|} $$
</div>
<p>PQ is computed per-category and results are averaged across categories. Predicted segments that have significant overlaps with unlabeled or crowd regions are filtered out from the FP set. More details of the metric can be found in the paper. Finally, PQ can be decomposed as the multiplication of a segmentation quality (SQ) term and a recognition quality (RQ) term. The decomposition of PQ = SQ * RQ is useful for providing additional insights for analysis:</p>
<div class="json fontMono">
  $$ {\text{PQ}} = \underbrace{\frac{\sum_{(p, g) \in TP} \text{IoU}(p, g)}{\vphantom{\frac{1}{2}}|TP|}}_{\text{segmentation quality (SQ)}} \times \underbrace{\frac{|TP|}{|TP| + \frac{1}{2} |FP| + \frac{1}{2} |FN|}}_{\text{recognition quality (RQ)}} $$
</div>
<p>For more details about the panoptic task and metrics please see the <a href="https://arxiv.org/abs/1801.00868">panoptic segmentation paper</a>.</p>

<h1>3. Metrics</h1>
<p>The following 9 metrics are used for characterizing the performance of a panoptic segmenter on COCO:</p>
<div class="json fontMono">
  <div class="jsonreg"><b>Average Panoptic Metrics:</b></div>
  <div class="jsonk">PQ</div><div class="jsonv">% Panoptic Quality <b>(primary challenge metric)</b></div>
  <div class="jsonk">SQ</div><div class="jsonv">% Segmentation Quality component of PQ</div>
  <div class="jsonk">RQ</div><div class="jsonv">% Recognition Quality component of PQ</div>
  <div class="jsonreg"><b>Panoptic Metrics for Things Categories:</b></div>
  <div class="jsonk">PQ<sup>Th</sup></div><div class="jsonv">% PQ for things categories only</div>
  <div class="jsonk">SQ<sup>Th</sup></div><div class="jsonv">% SQ for things categories only</div>
  <div class="jsonk">RQ<sup>Th</sup></div><div class="jsonv">% RQ for over things categories only</div>
  <div class="jsonreg"><b>Panoptic Metrics for Stuff Categories:</b></div>
  <div class="jsonk">PQ<sup>St</sup></div><div class="jsonv">% PQ for stuff categories only</div>
  <div class="jsonk">SQ<sup>St</sup></div><div class="jsonv">% SQ for stuff categories only</div>
  <div class="jsonk">RQ<sup>St</sup></div><div class="jsonv">% RQ for stuff categories only</div>
</div></br>
<ol>
  <li>PQ will determine the challenge winner. Remaining metrics are reported for analysis purposes.</li>
  <li>PQ (and likewise SQ and RQ) are averaged over all categories, including both stuff and thing categories.</li>
  <li>Please note that per-category, PQ = SQ * RQ (but averages across categories will not decompose this way).</li>
</ol>

<h1>4. Evaluation Code</h1>
<p>We provide extensive API support for the COCO images, annotations, and evaluation code. To download the COCO Panoptic API, please visit our <a href="https://github.com/cocodataset/panopticapi">GitHub repo</a>. Please note that the main COCO API does not support panoptic annotations.</p>

<script type="text/javascript">
// forced refresh of MathJax to hopefully force equations to render properly
if(window.MathJax) { MathJax.Hub.Queue(["Typeset",MathJax.Hub]); }
</script>

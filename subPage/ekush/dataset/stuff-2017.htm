<h1>COCO 2017 Stuff Segmentation Task</h1>
<p align="center"><img src="images/stuff-splash.png" class="wide"/></p>
<p class="fontBig">The <a href="https://places-coco2017.github.io/#winners">Challenge Winners</a> have now been announced! Up-to-date results are on the <a href="#stuff-leaderboard">stuff leaderboard</a>. Note that the evaluation server on test-dev remains open for uploading of new results.</p>

<h1>1. Overview</h1>
<p>The COCO Stuff Segmentation Task is designed to push the state of the art in semantic segmentation of <i>stuff</i> classes. Whereas the <a href="#detection-2017">object detection</a> task addresses <i>thing</i> classes (person, car, elephant), this task focuses on <i>stuff</i> classes (grass, wall, sky). For full details of this task please see the <a href="#stuff-eval">stuff evaluation</a> page.</p>
<p>Things are objects with a specific size and shape, that are often composed of parts. Stuff classes are background materials that are defined by homogeneous or repetitive patterns of fine-scale properties, but have no specific or distinctive spatial extent or shape. Why the focus on stuff? Stuff covers about 66% of the pixels in COCO. It allows us to explain important aspects of an image, including scene type; which thing classes are likely to be present and their location; as well as geometric properties of the scene. The COCO Stuff Segmentation Task builds on the COCO-Stuff project as described on <a href="https://github.com/nightrome/cocostuff">this website</a> and in this <a href="https://arxiv.org/abs/1612.03716">research paper</a>. This task includes and extends the original dataset release. Please note that in order to scale annotation, stuff segmentations were collected on superpixel segmentations of an image.</p>
<p>This task is part of the <a href="https://places-coco2017.github.io/">Joint COCO and Places Recognition Challenge Workshop</a> at ICCV 2017. For further details about the joint workshop please visit the workshop page. Please also see the related COCO <a href="#detection-2017">detection</a> and <a href="#keypoints-2017">keypoint</a> tasks.</p>
<p>The task includes 55K COCO images (train 40K, val 5K, test-dev 5K, test-challenge 5K) with annotations for 91 stuff classes and 1 'other' class. The stuff annotations cover 38M superpixels (10B pixels) with 296K stuff regions (5.4 stuff labels per image). Annotations for train and val are now available for <a href="#download">download</a>, while test set annotations will remain private.</p>

<h1>2. Dates</h1>
<div class="json">
  <div class="jsonktxt fontBlue">October 8, 2017</div><div class="jsonvtxt">[Extended] Submission deadline (11:59 PST)</div>
  <div class="jsonktxt">October 15, 2017</div><div class="jsonvtxt">Challenge winners notified</div>
  <div class="jsonktxt">October 29, 2017</div><div class="jsonvtxt">Winners present at ICCV 2017 Workshop</div>
</div>

<h1>3. Organizers</h1>
<div>Holger Caesar (U. of Edinburgh)</div>
<div>Jasper Uijlings (Google)</div>
<div>Michael Maire (TTI-Chicago)</div>
<div>Tsung-Yi Lin (Cornell Tech)</div>
<div>Piotr Doll√°r (Facebook AI Research)</div>
<div>Vittorio Ferrari (Google, U. of Edinburgh)</div>

<h1>4. Award Committee</h1>
<div>Holger Caesar (U. of Edinburgh)</div>
<div>Jasper Uijlings (Google)</div>
<div>Michael Maire (TTI-Chicago)</div>
<div>Tsung-Yi Lin (Cornell Tech)</div>
<div>Genevieve Patterson (MSR)</div>
<div>Matteo Ruggero Ronchi (Caltech)</div>
<div>Yin Cui (Cornell Tech)</div>
<div>Serge Belongie (Cornell Tech)</div>
<div>Lubomir Bourdev (WaveOne, Inc.)</div>
<div>James Hays (Georgia Tech)</div>
<div>Pietro Perona (Caltech)</div>
<div>Deva Ramanan (CMU)</div>
<div>Vittorio Ferrari (Google, U. of Edinburgh)</div>

[{"leaderboard_id": 4, "date": "2015-08-02", "description": "Add an intermediate attributes layer between CNN and LSTM", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1506.01144", "team": {"id": 58, "members": "", "name": "ACVT"}, "results": "{\"CIDEr\": 0.924, \"Bleu_4\": 0.582, \"Bleu_3\": 0.694, \"Bleu_2\": 0.803, \"Bleu_1\": 0.892, \"ROUGE_L\": 0.672, \"METEOR\": 0.329, \"SPICE\": 0.616}", "results_details": null, "publication": "Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, What value high level concepts in vision to language problems?"}, {"leaderboard_id": 4, "date": "2015-05-29", "description": "This is the Google CNN + an LSTM from the CVPR 2015 paper.", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1411.4555", "team": {"id": 41, "members": "", "name": "Google"}, "results": "{\"CIDEr\": 0.946, \"Bleu_4\": 0.587, \"Bleu_3\": 0.694, \"Bleu_2\": 0.802, \"Bleu_1\": 0.895, \"ROUGE_L\": 0.682, \"METEOR\": 0.346, \"SPICE\": 0.636}", "results_details": null, "publication": "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, CVPR 2015"}, {"leaderboard_id": 4, "date": "2015-04-08", "description": "Initial output with VGG network", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1411.4952", "team": {"id": 43, "members": "", "name": "MSR"}, "results": "{\"CIDEr\": 0.925, \"Bleu_4\": 0.567, \"Bleu_3\": 0.678, \"Bleu_2\": 0.789, \"Bleu_1\": 0.88, \"ROUGE_L\": 0.662, \"METEOR\": 0.331, \"SPICE\": 0.627}", "results_details": null, "publication": "H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Doll\u00e1r, J. Gao, X. He, M. Mitchell, J. Platt, C.L. Zitnick, and G. Zweig, From Captions to Visual Concepts and Back, CVPR 2015"}, {"leaderboard_id": 4, "date": "2015-10-29", "description": "Attention model on attributes", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1603.03925", "team": {"id": 59, "members": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo", "name": "ATT"}, "results": "{\"CIDEr\": 0.958, \"Bleu_4\": 0.599, \"Bleu_3\": 0.709, \"Bleu_2\": 0.815, \"Bleu_1\": 0.9, \"ROUGE_L\": 0.682, \"METEOR\": 0.335, \"SPICE\": 0.631}", "results_details": null, "publication": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo, Image Captioning with Semantic Attention, CVPR 2016"}, {"leaderboard_id": 4, "date": "2015-03-23", "description": "Human baseline", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/pdf/1504.00325.pdf", "team": {"id": 44, "members": "", "name": "Human"}, "results": "{\"CIDEr\": 0.91, \"Bleu_4\": 0.471, \"Bleu_3\": 0.603, \"Bleu_2\": 0.744, \"Bleu_1\": 0.88, \"ROUGE_L\": 0.626, \"METEOR\": 0.335, \"SPICE\": 0.74}", "results_details": null, "publication": "Human Baseline"}, {"leaderboard_id": 4, "date": "2016-02-10", "description": "Combination of multiple neuraltalk based models using mutual evaluation.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 45, "members": "", "name": "PicSOM"}, "results": "{\"CIDEr\": 0.901, \"Bleu_4\": 0.57, \"Bleu_3\": 0.68, \"Bleu_2\": 0.792, \"Bleu_1\": 0.887, \"ROUGE_L\": 0.666, \"METEOR\": 0.328, \"SPICE\": 0.588}", "results_details": null, "publication": ""}, {"leaderboard_id": 4, "date": "2015-05-29", "description": "We train linear classifiers for bigrams that commonly occur in train dataset using PAAPL (Ushiku et al. : Efficient image annotation for automatic sentence generation).", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 46, "members": "", "name": "MIL"}, "results": "{\"CIDEr\": 0.69, \"Bleu_4\": 0.432, \"Bleu_3\": 0.564, \"Bleu_2\": 0.707, \"Bleu_1\": 0.827, \"ROUGE_L\": 0.596, \"METEOR\": 0.284, \"SPICE\": 0.532}", "results_details": null, "publication": ""}, {"leaderboard_id": 4, "date": "2015-12-08", "description": "[See LRCN paper for full description.] This is the &amp;amp;amp;quot;two layer, factored&amp;amp;amp;quot; variant of the LRCN architecture.  We used VGGNet as the base visual network.  Sentences are generated using beam search with beam size 1.", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1411.4389", "team": {"id": 55, "members": "", "name": "Berkeley LRCN"}, "results": "{\"CIDEr\": 0.934, \"Bleu_4\": 0.585, \"Bleu_3\": 0.695, \"Bleu_2\": 0.804, \"Bleu_1\": 0.895, \"ROUGE_L\": 0.678, \"METEOR\": 0.335, \"SPICE\": 0.599}", "results_details": null, "publication": "J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR 2015"}, {"leaderboard_id": 4, "date": "2015-04-10", "description": "Our submission is an ensemble of 4 multimodal log-bilinear models [1,2] whose hyperparameters are tuned using Bayesian optimization with DNGO (Deep networks for global optimization) [3]. A beam search is used for generation.", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/pdf/1411.2539.pdf", "team": {"id": 47, "members": "", "name": "MLBL"}, "results": "{\"CIDEr\": 0.752, \"Bleu_4\": 0.517, \"Bleu_3\": 0.633, \"Bleu_2\": 0.747, \"Bleu_1\": 0.848, \"ROUGE_L\": 0.635, \"METEOR\": 0.294, \"SPICE\": 0.51}", "results_details": null, "publication": "Multimodal Neural Language Models (Kiros et al, ICML 2014), Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models (Kiros et al, arXiv 2014), and Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al, arXiv 2015)"}, {"leaderboard_id": 4, "date": "2015-05-15", "description": "k-Nearest Neighbor approach.", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1505.04467", "team": {"id": 48, "members": "", "name": "Nearest Neighbor"}, "results": "{\"CIDEr\": 0.916, \"Bleu_4\": 0.542, \"Bleu_3\": 0.655, \"Bleu_2\": 0.77, \"Bleu_1\": 0.872, \"ROUGE_L\": 0.648, \"METEOR\": 0.318, \"SPICE\": 0.593}", "results_details": null, "publication": "Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv 2015"}, {"leaderboard_id": 4, "date": "2015-05-28", "description": "Maximum entropy language model + DMSM + gated recurrent neural network + part-of-speech features + cider tuning", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1505.01809", "team": {"id": 49, "members": "", "name": "MSR Captivator"}, "results": "{\"CIDEr\": 0.937, \"Bleu_4\": 0.601, \"Bleu_3\": 0.71, \"Bleu_2\": 0.819, \"Bleu_1\": 0.907, \"ROUGE_L\": 0.68, \"METEOR\": 0.339, \"SPICE\": 0.609}", "results_details": null, "publication": "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv 2015"}, {"leaderboard_id": 4, "date": "2015-05-30", "description": "This is an updated version of the m-RNN model. ", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1504.06692", "team": {"id": 50, "members": "", "name": "m-RNN"}, "results": "{\"CIDEr\": 0.935, \"Bleu_4\": 0.575, \"Bleu_3\": 0.687, \"Bleu_2\": 0.798, \"Bleu_1\": 0.89, \"ROUGE_L\": 0.666, \"METEOR\": 0.325, \"SPICE\": 0.6}", "results_details": null, "publication": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images. arXiv preprint arXiv:1504.06692 (2015) "}, {"leaderboard_id": 4, "date": "2015-07-12", "description": "A query expansion-based image captioning method", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 60, "members": "", "name": "HUCVL-METULcsL"}, "results": "{\"CIDEr\": 0.399, \"Bleu_4\": 0.202, \"Bleu_3\": 0.306, \"Bleu_2\": 0.452, \"Bleu_1\": 0.639, \"ROUGE_L\": 0.46, \"METEOR\": 0.23, \"SPICE\": 0.479}", "results_details": null, "publication": ""}, {"leaderboard_id": 4, "date": "2015-05-31", "description": "An ensemble of 7 soft attention models.", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1502.03044 ", "team": {"id": 51, "members": "", "name": "Montreal/Toronto"}, "results": "{\"CIDEr\": 0.893, \"Bleu_4\": 0.537, \"Bleu_3\": 0.658, \"Bleu_2\": 0.779, \"Bleu_1\": 0.881, \"ROUGE_L\": 0.654, \"METEOR\": 0.322, \"SPICE\": 0.598}", "results_details": null, "publication": "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, arXiv 2015"}, {"leaderboard_id": 4, "date": "2016-04-23", "description": "Multimodal Recurrent Neural Network (LSTMs)", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 61, "members": "", "name": "Snapshopr_Research"}, "results": "{\"CIDEr\": 0.826, \"Bleu_4\": 0.493, \"Bleu_3\": 0.615, \"Bleu_2\": 0.743, \"Bleu_1\": 0.856, \"ROUGE_L\": 0.636, \"METEOR\": 0.308, \"SPICE\": 0.57}", "results_details": null, "publication": ""}, {"leaderboard_id": 4, "date": "2015-08-21", "description": "Add a gated function for image features on deep RNN.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 62, "members": "", "name": "Shijian Tang"}, "results": "{\"CIDEr\": 0.685, \"Bleu_4\": 0.465, \"Bleu_3\": 0.59, \"Bleu_2\": 0.72, \"Bleu_1\": 0.836, \"ROUGE_L\": 0.61, \"METEOR\": 0.274, \"SPICE\": 0.462}", "results_details": null, "publication": ""}, {"leaderboard_id": 4, "date": "2015-09-24", "description": "An alternate of long Short term memory, where the memory is semantically overlooked.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 63, "members": "", "name": "GabYesh"}, "results": "{\"CIDEr\": 0.443, \"Bleu_4\": 0.323, \"Bleu_3\": 0.427, \"Bleu_2\": 0.564, \"Bleu_1\": 0.725, \"ROUGE_L\": 0.524, \"METEOR\": 0.219, \"SPICE\": 0.344}", "results_details": null, "publication": ""}, {"leaderboard_id": 4, "date": "2015-04-15", "description": "One layer Vanilla LSTM with 512 hidden units, 512-D words vectors trained from scratch. Beam size 1. Trained with RMSProp, gradient clipping.", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1412.2306", "team": {"id": 52, "members": "", "name": "NeuralTalk"}, "results": "{\"CIDEr\": 0.692, \"Bleu_4\": 0.446, \"Bleu_3\": 0.566, \"Bleu_2\": 0.701, \"Bleu_1\": 0.828, \"ROUGE_L\": 0.603, \"METEOR\": 0.28, \"SPICE\": 0.496}", "results_details": null, "publication": "Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Descriptions, CVPR 2015"}, {"leaderboard_id": 4, "date": "2015-05-29", "description": "disclosed in future publication", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1506.03995 ", "team": {"id": 53, "members": "", "name": "Brno University"}, "results": "{\"CIDEr\": 0.536, \"Bleu_4\": 0.278, \"Bleu_3\": 0.392, \"Bleu_2\": 0.541, \"Bleu_1\": 0.716, \"ROUGE_L\": 0.509, \"METEOR\": 0.252, \"SPICE\": 0.505}", "results_details": null, "publication": "Martin Kolar, Michal Hradis, Pavel Zemcik, Technical Report: Image Captioning with Semantically Similar Images, arXiv 2015"}, {"leaderboard_id": 4, "date": "2015-05-26", "description": "This is an updated version of the m-RNN model. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. We only use MS COCO data. More details can be found at http://www.stat.ucla.edu/~junhua.mao/m-RNN.html", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1412.6632", "team": {"id": 54, "members": "", "name": "m-RNN (Baidu/ UCLA)"}, "results": "{\"CIDEr\": 0.896, \"Bleu_4\": 0.578, \"Bleu_3\": 0.69, \"Bleu_2\": 0.801, \"Bleu_1\": 0.89, \"ROUGE_L\": 0.668, \"METEOR\": 0.32, \"SPICE\": 0.591}", "results_details": null, "publication": "Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan, Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN), arXiv 2014"}, {"leaderboard_id": 4, "date": "2016-03-03", "description": "In order to loss of image information and overfitting, we proposed a novel modified LSTM to handle input image at every time step.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 264, "members": "", "name": "nlab-utokyo"}, "results": "{\"CIDEr\": 0.851, \"Bleu_4\": 0.518, \"Bleu_3\": 0.638, \"Bleu_2\": 0.762, \"Bleu_1\": 0.871, \"ROUGE_L\": 0.646, \"METEOR\": 0.316, \"SPICE\": 0.588}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-03-29", "description": "Use Deeper n/w with residual connections for the language model", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 267, "members": "", "name": "AugmentCNNwithDet"}, "results": "{\"CIDEr\": 0.968, \"Bleu_4\": 0.597, \"Bleu_3\": 0.706, \"Bleu_2\": 0.815, \"Bleu_1\": 0.905, \"ROUGE_L\": 0.683, \"METEOR\": 0.34, \"SPICE\": 0.612}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-10-25", "description": "Image captioning by exploiting image attributes. ", "leaderboard_name": "cap-c40", "url": "https://arxiv.org/abs/1611.01646", "team": {"id": 269, "members": "", "name": "MSM@MSRA"}, "results": "{\"CIDEr\": 1.053, \"Bleu_4\": 0.646, \"Bleu_3\": 0.751, \"Bleu_2\": 0.851, \"Bleu_1\": 0.926, \"ROUGE_L\": 0.709, \"METEOR\": 0.361, \"SPICE\": 0.669}", "results_details": "", "publication": "Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, Tao Mei, Boosting Image Captioning with Attributes, arXiv 2016"}, {"leaderboard_id": 4, "date": "2016-05-25", "description": "an LSTM autoencoder with a recurrent spatial transformer is used to encode CNN features and obtain a representation, and then this representation is decoded into a language description.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 270, "members": "", "name": "zhaofang_lv"}, "results": "{\"CIDEr\": 0.917, \"Bleu_4\": 0.618, \"Bleu_3\": 0.727, \"Bleu_2\": 0.828, \"Bleu_1\": 0.906, \"ROUGE_L\": 0.686, \"METEOR\": 0.335, \"SPICE\": 0.584}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-03-13", "description": "Sequential Image Captioner, ascending order", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 271, "members": "", "name": "MSRA-MSM"}, "results": "{\"CIDEr\": 0.954, \"Bleu_4\": 0.591, \"Bleu_3\": 0.705, \"Bleu_2\": 0.815, \"Bleu_1\": 0.901, \"ROUGE_L\": 0.677, \"METEOR\": 0.328, \"SPICE\": 0.622}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-06-08", "description": "Weight the words and use the weights when training", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 272, "members": "", "name": "THU_MIG"}, "results": "{\"CIDEr\": 1.013, \"Bleu_4\": 0.614, \"Bleu_3\": 0.731, \"Bleu_2\": 0.839, \"Bleu_1\": 0.917, \"ROUGE_L\": 0.685, \"METEOR\": 0.336, \"SPICE\": 0.656}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-05-21", "description": "Resnet-based captioning", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 274, "members": "", "name": "ChalLS"}, "results": "{\"CIDEr\": 0.97, \"Bleu_4\": 0.59, \"Bleu_3\": 0.701, \"Bleu_2\": 0.809, \"Bleu_1\": 0.898, \"ROUGE_L\": 0.679, \"METEOR\": 0.34, \"SPICE\": 0.638}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-11-14", "description": "e2e-glstm-tc5", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 694, "members": "", "name": "e2e-glstm-tc5"}, "results": "{\"CIDEr\": 0.929, \"Bleu_4\": 0.569, \"Bleu_3\": 0.682, \"Bleu_2\": 0.794, \"Bleu_1\": 0.887, \"ROUGE_L\": 0.667, \"METEOR\": 0.33, \"SPICE\": 0.618}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-11-06", "description": "rnn with semantic embedding", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 695, "members": "", "name": "feng"}, "results": "{\"CIDEr\": 1.002, \"Bleu_4\": 0.621, \"Bleu_3\": 0.735, \"Bleu_2\": 0.84, \"Bleu_1\": 0.917, \"ROUGE_L\": 0.691, \"METEOR\": 0.343, \"SPICE\": 0.635}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-11-15", "description": "We use SCA-CNN model trained only in train2014 dataset", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 697, "members": "", "name": "Next"}, "results": "{\"CIDEr\": 0.921, \"Bleu_4\": 0.579, \"Bleu_3\": 0.691, \"Bleu_2\": 0.802, \"Bleu_1\": 0.894, \"ROUGE_L\": 0.674, \"METEOR\": 0.331, \"SPICE\": 0.588}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-12-03", "description": "attentive model with semantic features and external corpus", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 698, "members": "", "name": "ATT_VC_REG"}, "results": "{\"CIDEr\": 0.974, \"Bleu_4\": 0.602, \"Bleu_3\": 0.715, \"Bleu_2\": 0.823, \"Bleu_1\": 0.91, \"ROUGE_L\": 0.686, \"METEOR\": 0.341, \"SPICE\": 0.625}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-07-23", "description": "sentence conditional semantic attention method, as introduced in:  http://arxiv.org/abs/1606.04621", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1606.04621", "team": {"id": 700, "members": "", "name": "UMich-COG"}, "results": "{\"CIDEr\": 0.913, \"Bleu_4\": 0.565, \"Bleu_3\": 0.678, \"Bleu_2\": 0.793, \"Bleu_1\": 0.888, \"ROUGE_L\": 0.666, \"METEOR\": 0.329, \"SPICE\": 0.612}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-07-28", "description": "ResCeption (inception-v3 + ResNet) plus Dropout-LSTM plus beam-search2", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 701, "members": "Taewan Ethan Kim", "name": "TaewanEthanKim"}, "results": "{\"CIDEr\": 0.937, \"Bleu_4\": 0.568, \"Bleu_3\": 0.681, \"Bleu_2\": 0.795, \"Bleu_1\": 0.888, \"ROUGE_L\": 0.671, \"METEOR\": 0.333, \"SPICE\": 0.61}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-06-13", "description": "sentence guided attention model using k most consensus captions", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 702, "members": "", "name": "Postech_CV"}, "results": "{\"CIDEr\": 1.001, \"Bleu_4\": 0.607, \"Bleu_3\": 0.722, \"Bleu_2\": 0.832, \"Bleu_1\": 0.915, \"ROUGE_L\": 0.686, \"METEOR\": 0.341, \"SPICE\": 0.642}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-10-24", "description": "review network", "leaderboard_name": "cap-c40", "url": "https://arxiv.org/abs/1605.07912", "team": {"id": 703, "members": "", "name": "reviewnet"}, "results": "{\"CIDEr\": 0.969, \"Bleu_4\": 0.597, \"Bleu_3\": 0.705, \"Bleu_2\": 0.812, \"Bleu_1\": 0.9, \"ROUGE_L\": 0.686, \"METEOR\": 0.347, \"SPICE\": 0.649}", "results_details": "", "publication": "Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W. Cohen, Review Networks for Caption Generation, NIPS 2016."}, {"leaderboard_id": 4, "date": "2016-11-28", "description": "Attention semantic featuers with external textual data pretraining", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 705, "members": "", "name": "Dalab_Master_Thesis"}, "results": "{\"CIDEr\": 0.978, \"Bleu_4\": 0.6, \"Bleu_3\": 0.714, \"Bleu_2\": 0.823, \"Bleu_1\": 0.91, \"ROUGE_L\": 0.685, \"METEOR\": 0.34, \"SPICE\": 0.629}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-11-22", "description": "The IC-SCA model applies a Long Short-Term Memory (LSTM) to predict the synchronous attention, receiving shared representation between vision and text. That attention is then applied to guide a guiding LSTM (gLSTM) for description generation.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 706, "members": "", "name": "AI_BUPT"}, "results": "{\"CIDEr\": 0.903, \"Bleu_4\": 0.555, \"Bleu_3\": 0.668, \"Bleu_2\": 0.784, \"Bleu_1\": 0.881, \"ROUGE_L\": 0.663, \"METEOR\": 0.327, \"SPICE\": 0.599}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-03-17", "description": "Attention models trained with reinforcement learning.", "leaderboard_name": "cap-c40", "url": "https://arxiv.org/abs/1612.00563", "team": {"id": 707, "members": "", "name": "Watson Multimodal"}, "results": "{\"CIDEr\": 1.167, \"Bleu_4\": 0.645, \"Bleu_3\": 0.759, \"Bleu_2\": 0.86, \"Bleu_1\": 0.931, \"ROUGE_L\": 0.707, \"METEOR\": 0.355, \"SPICE\": 0.689}", "results_details": "", "publication": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, Vaibhava Goel, Self-critical Sequence Training for Image Captioning, arXiv 2016"}, {"leaderboard_id": 4, "date": "2016-09-04", "description": "trnn", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 708, "members": "", "name": "DLTC@MSR"}, "results": "{\"CIDEr\": 1.013, \"Bleu_4\": 0.631, \"Bleu_3\": 0.739, \"Bleu_2\": 0.839, \"Bleu_1\": 0.917, \"ROUGE_L\": 0.696, \"METEOR\": 0.348, \"SPICE\": 0.641}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-11-22", "description": "Use Skeleton as a key", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 709, "members": "", "name": "DONOT_FAIL_AGAIN"}, "results": "{\"CIDEr\": 1.026, \"Bleu_4\": 0.612, \"Bleu_3\": 0.724, \"Bleu_2\": 0.829, \"Bleu_1\": 0.912, \"ROUGE_L\": 0.698, \"METEOR\": 0.355, \"SPICE\": 0.683}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-05-09", "description": "Generate captions with visual attention on flexible regions.", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1506.06272", "team": {"id": 710, "members": "", "name": "TsinghuaBigeye"}, "results": "{\"CIDEr\": 0.946, \"Bleu_4\": 0.601, \"Bleu_3\": 0.711, \"Bleu_2\": 0.817, \"Bleu_1\": 0.902, \"ROUGE_L\": 0.68, \"METEOR\": 0.336, \"SPICE\": 0.613}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-06-13", "description": "Image captioning by exploiting image attributes. ", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 712, "members": "", "name": "ATT-IMG (MSM@MSRA)"}, "results": "{\"CIDEr\": 1.036, \"Bleu_4\": 0.645, \"Bleu_3\": 0.752, \"Bleu_2\": 0.852, \"Bleu_1\": 0.926, \"ROUGE_L\": 0.707, \"METEOR\": 0.356, \"SPICE\": 0.653}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-12-05", "description": "Implement CMIC", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 939, "members": "", "name": "3D&amp;CV"}, "results": "{\"CIDEr\": 0.938, \"Bleu_4\": 0.568, \"Bleu_3\": 0.678, \"Bleu_2\": 0.791, \"Bleu_1\": 0.888, \"ROUGE_L\": 0.674, \"METEOR\": 0.339, \"SPICE\": 0.62}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-11-29", "description": "CNN_RNN", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 940, "members": "", "name": "Lingxiang"}, "results": "{\"CIDEr\": 0.901, \"Bleu_4\": 0.548, \"Bleu_3\": 0.662, \"Bleu_2\": 0.779, \"Bleu_1\": 0.879, \"ROUGE_L\": 0.662, \"METEOR\": 0.328, \"SPICE\": 0.605}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2016-10-30", "description": "Optimization of image description metrics using policy gradient methods.", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1612.00370", "team": {"id": 942, "members": "", "name": "G-RMI (PG-BCMR)"}, "results": "{\"CIDEr\": 1.032, \"Bleu_4\": 0.624, \"Bleu_3\": 0.738, \"Bleu_2\": 0.841, \"Bleu_1\": 0.918, \"ROUGE_L\": 0.695, \"METEOR\": 0.34, \"SPICE\": 0.622}", "results_details": "", "publication": "Optimization of image description metrics using policy gradient methods, Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin"}, {"leaderboard_id": 4, "date": "2017-03-17", "description": "gg", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1612.00370", "team": {"id": 945, "members": "Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin ", "name": "G-RMI(PG-SPIDEr-TAG)"}, "results": "{\"CIDEr\": 0.983, \"Bleu_4\": 0.579, \"Bleu_3\": 0.698, \"Bleu_2\": 0.812, \"Bleu_1\": 0.898, \"ROUGE_L\": 0.676, \"METEOR\": 0.322, \"SPICE\": 0.572}", "results_details": "", "publication": "Optimization of image description metrics using policy gradient methods, Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin"}, {"leaderboard_id": 4, "date": "2016-12-01", "description": "", "leaderboard_name": "cap-c40", "url": "https://arxiv.org/abs/1612.01887", "team": {"id": 948, "members": "", "name": "MetaMind/VT_GT"}, "results": "{\"CIDEr\": 1.059, \"Bleu_4\": 0.637, \"Bleu_3\": 0.744, \"Bleu_2\": 0.845, \"Bleu_1\": 0.92, \"ROUGE_L\": 0.705, \"METEOR\": 0.359, \"SPICE\": 0.673}", "results_details": "", "publication": "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning, J. Lu*, C. Xiong*, D. Parikh and R. Socher"}, {"leaderboard_id": 4, "date": "2017-06-07", "description": "HEDN for image caption", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1244, "members": "", "name": "ucas_yu"}, "results": "{\"CIDEr\": 1.028, \"Bleu_4\": 0.628, \"Bleu_3\": 0.735, \"Bleu_2\": 0.836, \"Bleu_1\": 0.917, \"ROUGE_L\": 0.701, \"METEOR\": 0.357, \"SPICE\": 0.674}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2015-08-02", "description": "Add an intermediate attributes layer between CNN and LSTM", "leaderboard_name": "cap-c40", "url": "http://arxiv.org/abs/1506.01144", "team": {"id": 1246, "members": "Qi Wu\r", "name": "ACVT"}, "results": "{\"CIDEr\": 0.924, \"Bleu_4\": 0.582, \"Bleu_3\": 0.694, \"Bleu_2\": 0.803, \"Bleu_1\": 0.892, \"ROUGE_L\": 0.672, \"METEOR\": 0.329, \"SPICE\": 0.616}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-06-27", "description": "Actor-Critic", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1247, "members": "", "name": "QMUL-VISION"}, "results": "{\"CIDEr\": 1.121, \"Bleu_4\": 0.625, \"Bleu_3\": 0.745, \"Bleu_2\": 0.855, \"Bleu_1\": 0.929, \"ROUGE_L\": 0.691, \"METEOR\": 0.344, \"SPICE\": 0.68}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-02-11", "description": "Multimodal Attentive Translator", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1248, "members": "", "name": "LC-JHU"}, "results": "{\"CIDEr\": 1.064, \"Bleu_4\": 0.617, \"Bleu_3\": 0.727, \"Bleu_2\": 0.831, \"Bleu_1\": 0.911, \"ROUGE_L\": 0.691, \"METEOR\": 0.348, \"SPICE\": 0.657}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-07-22", "description": "Bottom-Up and Top-Down Attention", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1249, "members": "", "name": "panderson@MSR/ACRV"}, "results": "{\"CIDEr\": 1.205, \"Bleu_4\": 0.685, \"Bleu_3\": 0.794, \"Bleu_2\": 0.888, \"Bleu_1\": 0.952, \"ROUGE_L\": 0.724, \"METEOR\": 0.367, \"SPICE\": 0.715}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-07-22", "description": "CASIA_IVA_DA", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1250, "members": "", "name": "CASIA_IVA"}, "results": "{\"CIDEr\": 1.188, \"Bleu_4\": 0.669, \"Bleu_3\": 0.776, \"Bleu_2\": 0.87, \"Bleu_1\": 0.934, \"ROUGE_L\": 0.719, \"METEOR\": 0.362, \"SPICE\": 0.702}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-06-11", "description": "Encode image features with CNN and then decode them with an LSTM network.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1251, "members": "", "name": "DeepDzayer"}, "results": "{\"CIDEr\": 0.749, \"Bleu_4\": 0.454, \"Bleu_3\": 0.582, \"Bleu_2\": 0.721, \"Bleu_1\": 0.842, \"ROUGE_L\": 0.61, \"METEOR\": 0.285, \"SPICE\": 0.514}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-08-07", "description": "multi-attention and RL.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1253, "members": "", "name": "TencentVision"}, "results": "{\"CIDEr\": 1.224, \"Bleu_4\": 0.673, \"Bleu_3\": 0.786, \"Bleu_2\": 0.884, \"Bleu_1\": 0.947, \"ROUGE_L\": 0.722, \"METEOR\": 0.366, \"SPICE\": 0.704}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-01-21", "description": "mil_vec+2-lstm", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1254, "members": "", "name": "chapternew"}, "results": "{\"CIDEr\": 0.884, \"Bleu_4\": 0.528, \"Bleu_3\": 0.647, \"Bleu_2\": 0.769, \"Bleu_1\": 0.874, \"ROUGE_L\": 0.653, \"METEOR\": 0.325, \"SPICE\": 0.609}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-03-15", "description": "Unified attributes network for image captioning ", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1255, "members": "", "name": "HUST_2017"}, "results": "{\"CIDEr\": 0.989, \"Bleu_4\": 0.611, \"Bleu_3\": 0.722, \"Bleu_2\": 0.827, \"Bleu_1\": 0.91, \"ROUGE_L\": 0.691, \"METEOR\": 0.345, \"SPICE\": 0.644}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-07-22", "description": "dascup", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1256, "members": "", "name": "DEEPAI"}, "results": "{\"CIDEr\": 1.194, \"Bleu_4\": 0.67, \"Bleu_3\": 0.778, \"Bleu_2\": 0.871, \"Bleu_1\": 0.935, \"ROUGE_L\": 0.721, \"METEOR\": 0.364, \"SPICE\": 0.711}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-03-28", "description": "mil+att+lstm", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1258, "members": "", "name": "zhangjie"}, "results": "{\"CIDEr\": 0.889, \"Bleu_4\": 0.534, \"Bleu_3\": 0.654, \"Bleu_2\": 0.776, \"Bleu_1\": 0.878, \"ROUGE_L\": 0.656, \"METEOR\": 0.324, \"SPICE\": 0.606}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-05-31", "description": "RNN+LCNN (test)", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1259, "members": "", "name": "rose"}, "results": "{\"CIDEr\": 0.925, \"Bleu_4\": 0.578, \"Bleu_3\": 0.692, \"Bleu_2\": 0.804, \"Bleu_1\": 0.894, \"ROUGE_L\": 0.672, \"METEOR\": 0.331, \"SPICE\": 0.612}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-03-24", "description": "Caption model", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1260, "members": "", "name": "kboris_plapko_viktor"}, "results": "{\"CIDEr\": 0.451, \"Bleu_4\": 0.351, \"Bleu_3\": 0.464, \"Bleu_2\": 0.605, \"Bleu_1\": 0.758, \"ROUGE_L\": 0.547, \"METEOR\": 0.233, \"SPICE\": 0.356}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-03-15", "description": "mixer-adam-bcmr", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1261, "members": "", "name": "G"}, "results": "{\"CIDEr\": 1.012, \"Bleu_4\": 0.6, \"Bleu_3\": 0.718, \"Bleu_2\": 0.827, \"Bleu_1\": 0.909, \"ROUGE_L\": 0.686, \"METEOR\": 0.34, \"SPICE\": 0.645}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-04-29", "description": "attentive linear transformation", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1262, "members": "", "name": "SenmaoYe"}, "results": "{\"CIDEr\": 1.059, \"Bleu_4\": 0.639, \"Bleu_3\": 0.743, \"Bleu_2\": 0.843, \"Bleu_1\": 0.922, \"ROUGE_L\": 0.712, \"METEOR\": 0.37, \"SPICE\": 0.692}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-07-17", "description": "GOOG+LSTM", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1263, "members": "", "name": "dreamteam"}, "results": "{\"CIDEr\": 0.816, \"Bleu_4\": 0.517, \"Bleu_3\": 0.633, \"Bleu_2\": 0.754, \"Bleu_1\": 0.86, \"ROUGE_L\": 0.641, \"METEOR\": 0.308, \"SPICE\": 0.547}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-03-30", "description": "USING ATTRIBUTE AND ATTENTION&amp;GUIDED", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1264, "members": "", "name": "SCU-IMAGE"}, "results": "{\"CIDEr\": 0.907, \"Bleu_4\": 0.577, \"Bleu_3\": 0.687, \"Bleu_2\": 0.801, \"Bleu_1\": 0.898, \"ROUGE_L\": 0.675, \"METEOR\": 0.339, \"SPICE\": 0.594}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-01-12", "description": "Image captioning with semantic attributes", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1265, "members": "", "name": "SRCB@Ricoh"}, "results": "{\"CIDEr\": 1.047, \"Bleu_4\": 0.651, \"Bleu_3\": 0.756, \"Bleu_2\": 0.855, \"Bleu_1\": 0.928, \"ROUGE_L\": 0.711, \"METEOR\": 0.362, \"SPICE\": 0.664}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-03-27", "description": "methods based on RNN and learning to form guiding information", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1266, "members": "", "name": "TencentVision"}, "results": "{\"CIDEr\": 1.035, \"Bleu_4\": 0.64, \"Bleu_3\": 0.747, \"Bleu_2\": 0.847, \"Bleu_1\": 0.923, \"ROUGE_L\": 0.705, \"METEOR\": 0.359, \"SPICE\": 0.67}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-05-08", "description": "tags", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1267, "members": "", "name": "UPC2017"}, "results": "{\"CIDEr\": 1.02, \"Bleu_4\": 0.615, \"Bleu_3\": 0.725, \"Bleu_2\": 0.829, \"Bleu_1\": 0.91, \"ROUGE_L\": 0.695, \"METEOR\": 0.351, \"SPICE\": 0.656}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-06-13", "description": "CNN with LSTM", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1268, "members": "", "name": "Lingxiang_UTS"}, "results": "{\"CIDEr\": 0.972, \"Bleu_4\": 0.577, \"Bleu_3\": 0.689, \"Bleu_2\": 0.801, \"Bleu_1\": 0.895, \"ROUGE_L\": 0.679, \"METEOR\": 0.343, \"SPICE\": 0.653}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-08-02", "description": "first detect words, then generate caption", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1269, "members": "", "name": "bmc-uestc"}, "results": "{\"CIDEr\": 1.046, \"Bleu_4\": 0.642, \"Bleu_3\": 0.749, \"Bleu_2\": 0.85, \"Bleu_1\": 0.926, \"ROUGE_L\": 0.71, \"METEOR\": 0.364, \"SPICE\": 0.695}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-05-31", "description": "Resnet101+Language CNN+RNN", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1270, "members": "", "name": "NTU_ROSE_CAP"}, "results": "{\"CIDEr\": 1.025, \"Bleu_4\": 0.586, \"Bleu_3\": 0.709, \"Bleu_2\": 0.827, \"Bleu_1\": 0.918, \"ROUGE_L\": 0.68, \"METEOR\": 0.336, \"SPICE\": 0.657}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-02-24", "description": "an improvement of im2txt", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1271, "members": "", "name": "Kyle Yang"}, "results": "{\"CIDEr\": 0.936, \"Bleu_4\": 0.578, \"Bleu_3\": 0.686, \"Bleu_2\": 0.796, \"Bleu_1\": 0.89, \"ROUGE_L\": 0.677, \"METEOR\": 0.339, \"SPICE\": 0.607}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-05-29", "description": "The method is based on a new framework which combines objects attention with attributes", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1272, "members": "", "name": "licongthu"}, "results": "{\"CIDEr\": 0.972, \"Bleu_4\": 0.599, \"Bleu_3\": 0.71, \"Bleu_2\": 0.819, \"Bleu_1\": 0.908, \"ROUGE_L\": 0.683, \"METEOR\": 0.34, \"SPICE\": 0.627}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-02-02", "description": "hitachi", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1273, "members": "", "name": "hitachi"}, "results": "{\"CIDEr\": 0.843, \"Bleu_4\": 0.499, \"Bleu_3\": 0.62, \"Bleu_2\": 0.746, \"Bleu_1\": 0.857, \"ROUGE_L\": 0.631, \"METEOR\": 0.308, \"SPICE\": 0.566}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-06-13", "description": "We firstly detect words form image, then generate captions using detected words.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1274, "members": "", "name": "CAP_BMC"}, "results": "{\"CIDEr\": 1.047, \"Bleu_4\": 0.645, \"Bleu_3\": 0.749, \"Bleu_2\": 0.848, \"Bleu_1\": 0.924, \"ROUGE_L\": 0.71, \"METEOR\": 0.365, \"SPICE\": 0.693}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-07-31", "description": "composed GAN and cider", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1275, "members": "", "name": "DCAIL04"}, "results": "{\"CIDEr\": 0.967, \"Bleu_4\": 0.579, \"Bleu_3\": 0.697, \"Bleu_2\": 0.814, \"Bleu_1\": 0.905, \"ROUGE_L\": 0.662, \"METEOR\": 0.316, \"SPICE\": 0.548}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-08-02", "description": "image caption with Part of Speech Guided", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1287, "members": "", "name": "bai&amp;amp;#39;s la"}, "results": "{\"CIDEr\": 0.951, \"Bleu_4\": 0.603, \"Bleu_3\": 0.715, \"Bleu_2\": 0.819, \"Bleu_1\": 0.904, \"ROUGE_L\": 0.683, \"METEOR\": 0.336, \"SPICE\": 0.619}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2015-10-29", "description": "", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1289, "members": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo", "name": ""}, "results": "{\"CIDEr\": 0.958, \"Bleu_4\": 0.599, \"Bleu_3\": 0.709, \"Bleu_2\": 0.815, \"Bleu_1\": 0.9, \"ROUGE_L\": 0.682, \"METEOR\": 0.335, \"SPICE\": 0.631}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-02-09", "description": "Implement CMIC", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1290, "members": "", "name": "3D&amp;amp;CV"}, "results": "{\"CIDEr\": 0.963, \"Bleu_4\": 0.579, \"Bleu_3\": 0.688, \"Bleu_2\": 0.799, \"Bleu_1\": 0.894, \"ROUGE_L\": 0.68, \"METEOR\": 0.343, \"SPICE\": 0.632}", "results_details": "", "publication": ""}, {"leaderboard_id": 4, "date": "2017-07-29", "description": "A structured semantic embedding method for image captioning, which employs visual semantic parsing tree and structured semantic attention.", "leaderboard_name": "cap-c40", "url": "", "team": {"id": 1292, "members": "Media Analysis and Computing Lab, Xiamen university\r", "name": "XMU&amp;TencentYouTu"}, "results": "{\"CIDEr\": 0.948, \"Bleu_4\": 0.607, \"Bleu_3\": 0.717, \"Bleu_2\": 0.822, \"Bleu_1\": 0.907, \"ROUGE_L\": 0.684, \"METEOR\": 0.34, \"SPICE\": 0.617}", "results_details": "", "publication": ""}]
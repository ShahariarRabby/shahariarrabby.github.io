[{"leaderboard_id": 5, "date": "2015-08-02", "description": "Add an intermediate attributes layer between CNN and LSTM", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1506.01144", "team": {"id": 58, "members": "", "name": "ACVT"}, "results": "{\"CIDEr\": 0.911, \"Bleu_4\": 0.306, \"Bleu_3\": 0.414, \"Bleu_2\": 0.556, \"Bleu_1\": 0.725, \"ROUGE_L\": 0.528, \"METEOR\": 0.246, \"SPICE\": 0.178}", "results_details": null, "publication": "Qi Wu, Chunhua Shen, Anton van den Hengel, Lingqiao Liu, Anthony Dick, What value high level concepts in vision to language problems?"}, {"leaderboard_id": 5, "date": "2015-05-29", "description": "This is the Google CNN + an LSTM from the CVPR 2015 paper.", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1411.4555", "team": {"id": 41, "members": "", "name": "Google"}, "results": "{\"CIDEr\": 0.943, \"Bleu_4\": 0.309, \"Bleu_3\": 0.407, \"Bleu_2\": 0.542, \"Bleu_1\": 0.713, \"ROUGE_L\": 0.53, \"METEOR\": 0.254, \"SPICE\": 0.182}", "results_details": null, "publication": "Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Show and Tell: A Neural Image Caption Generator, CVPR 2015"}, {"leaderboard_id": 5, "date": "2015-04-08", "description": "Initial output with VGG network", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1411.4952", "team": {"id": 43, "members": "", "name": "MSR"}, "results": "{\"CIDEr\": 0.912, \"Bleu_4\": 0.291, \"Bleu_3\": 0.391, \"Bleu_2\": 0.526, \"Bleu_1\": 0.695, \"ROUGE_L\": 0.519, \"METEOR\": 0.247, \"SPICE\": 0.186}", "results_details": null, "publication": "H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Doll\u00e1r, J. Gao, X. He, M. Mitchell, J. Platt, C.L. Zitnick, and G. Zweig, From Captions to Visual Concepts and Back, CVPR 2015"}, {"leaderboard_id": 5, "date": "2015-10-29", "description": "Attention model on attributes", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1603.03925", "team": {"id": 59, "members": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo", "name": "ATT"}, "results": "{\"CIDEr\": 0.943, \"Bleu_4\": 0.316, \"Bleu_3\": 0.424, \"Bleu_2\": 0.565, \"Bleu_1\": 0.731, \"ROUGE_L\": 0.535, \"METEOR\": 0.25, \"SPICE\": 0.182}", "results_details": null, "publication": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo, Image Captioning with Semantic Attention, CVPR 2016"}, {"leaderboard_id": 5, "date": "2015-03-23", "description": "Human baseline", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/pdf/1504.00325.pdf", "team": {"id": 44, "members": "", "name": "Human"}, "results": "{\"CIDEr\": 0.854, \"Bleu_4\": 0.217, \"Bleu_3\": 0.321, \"Bleu_2\": 0.469, \"Bleu_1\": 0.663, \"ROUGE_L\": 0.484, \"METEOR\": 0.252, \"SPICE\": 0.198}", "results_details": null, "publication": "Human Baseline"}, {"leaderboard_id": 5, "date": "2016-02-10", "description": "Combination of multiple neuraltalk based models using mutual evaluation.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 45, "members": "", "name": "PicSOM"}, "results": "{\"CIDEr\": 0.899, \"Bleu_4\": 0.299, \"Bleu_3\": 0.399, \"Bleu_2\": 0.536, \"Bleu_1\": 0.707, \"ROUGE_L\": 0.52, \"METEOR\": 0.243, \"SPICE\": 0.173}", "results_details": null, "publication": ""}, {"leaderboard_id": 5, "date": "2015-05-29", "description": "We train linear classifiers for bigrams that commonly occur in train dataset using PAAPL (Ushiku et al. : Efficient image annotation for automatic sentence generation).", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 46, "members": "", "name": "MIL"}, "results": "{\"CIDEr\": 0.666, \"Bleu_4\": 0.216, \"Bleu_3\": 0.323, \"Bleu_2\": 0.472, \"Bleu_1\": 0.652, \"ROUGE_L\": 0.468, \"METEOR\": 0.214, \"SPICE\": 0.151}", "results_details": null, "publication": ""}, {"leaderboard_id": 5, "date": "2015-12-08", "description": "[See LRCN paper for full description.] This is the &amp;amp;amp;quot;two layer, factored&amp;amp;amp;quot; variant of the LRCN architecture.  We used VGGNet as the base visual network.  Sentences are generated using beam search with beam size 1.", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1411.4389", "team": {"id": 55, "members": "", "name": "Berkeley LRCN"}, "results": "{\"CIDEr\": 0.921, \"Bleu_4\": 0.306, \"Bleu_3\": 0.409, \"Bleu_2\": 0.548, \"Bleu_1\": 0.718, \"ROUGE_L\": 0.528, \"METEOR\": 0.247, \"SPICE\": 0.177}", "results_details": null, "publication": "J. Donahue, L. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, T. Darrell, Long-term Recurrent Convolutional Networks for Visual Recognition and Description, CVPR 2015"}, {"leaderboard_id": 5, "date": "2015-04-10", "description": "Our submission is an ensemble of 4 multimodal log-bilinear models [1,2] whose hyperparameters are tuned using Bayesian optimization with DNGO (Deep networks for global optimization) [3]. A beam search is used for generation.", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/pdf/1411.2539.pdf", "team": {"id": 47, "members": "", "name": "MLBL"}, "results": "{\"CIDEr\": 0.74, \"Bleu_4\": 0.26, \"Bleu_3\": 0.362, \"Bleu_2\": 0.498, \"Bleu_1\": 0.666, \"ROUGE_L\": 0.499, \"METEOR\": 0.219, \"SPICE\": 0.151}", "results_details": null, "publication": "Multimodal Neural Language Models (Kiros et al, ICML 2014), Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models (Kiros et al, arXiv 2014), and Scalable Bayesian Optimization Using Deep Neural Networks (Snoek et al, arXiv 2015)"}, {"leaderboard_id": 5, "date": "2015-05-15", "description": "k-Nearest Neighbor approach.", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1505.04467", "team": {"id": 48, "members": "", "name": "Nearest Neighbor"}, "results": "{\"CIDEr\": 0.886, \"Bleu_4\": 0.28, \"Bleu_3\": 0.382, \"Bleu_2\": 0.521, \"Bleu_1\": 0.697, \"ROUGE_L\": 0.507, \"METEOR\": 0.237, \"SPICE\": 0.171}", "results_details": null, "publication": "Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick, Exploring Nearest Neighbor Approaches for Image Captioning, arXiv 2015"}, {"leaderboard_id": 5, "date": "2015-05-28", "description": "Maximum entropy language model + DMSM + gated recurrent neural network + part-of-speech features + cider tuning", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1505.01809", "team": {"id": 49, "members": "", "name": "MSR Captivator"}, "results": "{\"CIDEr\": 0.931, \"Bleu_4\": 0.308, \"Bleu_3\": 0.407, \"Bleu_2\": 0.543, \"Bleu_1\": 0.715, \"ROUGE_L\": 0.526, \"METEOR\": 0.248, \"SPICE\": 0.18}", "results_details": null, "publication": "Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell, Language Models for Image Captioning: The Quirks and What Works, arXiv 2015"}, {"leaderboard_id": 5, "date": "2015-05-30", "description": "This is an updated version of the m-RNN model. ", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1504.06692", "team": {"id": 50, "members": "", "name": "m-RNN"}, "results": "{\"CIDEr\": 0.917, \"Bleu_4\": 0.299, \"Bleu_3\": 0.404, \"Bleu_2\": 0.545, \"Bleu_1\": 0.716, \"ROUGE_L\": 0.521, \"METEOR\": 0.242, \"SPICE\": 0.174}", "results_details": null, "publication": "Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, and Alan Yuille, Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images. arXiv preprint arXiv:1504.06692 (2015) "}, {"leaderboard_id": 5, "date": "2015-07-12", "description": "A query expansion-based image captioning method", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 60, "members": "", "name": "HUCVL-METULcsL"}, "results": "{\"CIDEr\": 0.373, \"Bleu_4\": 0.089, \"Bleu_3\": 0.153, \"Bleu_2\": 0.263, \"Bleu_1\": 0.449, \"ROUGE_L\": 0.356, \"METEOR\": 0.175, \"SPICE\": 0.119}", "results_details": null, "publication": ""}, {"leaderboard_id": 5, "date": "2015-05-31", "description": "An ensemble of 7 soft attention models.", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1502.03044 ", "team": {"id": 51, "members": "", "name": "Montreal/Toronto"}, "results": "{\"CIDEr\": 0.865, \"Bleu_4\": 0.277, \"Bleu_3\": 0.383, \"Bleu_2\": 0.528, \"Bleu_1\": 0.705, \"ROUGE_L\": 0.516, \"METEOR\": 0.241, \"SPICE\": 0.172}", "results_details": null, "publication": "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, arXiv 2015"}, {"leaderboard_id": 5, "date": "2016-04-23", "description": "Multimodal Recurrent Neural Network (LSTMs)", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 61, "members": "", "name": "Snapshopr_Research"}, "results": "{\"CIDEr\": 0.791, \"Bleu_4\": 0.251, \"Bleu_3\": 0.353, \"Bleu_2\": 0.498, \"Bleu_1\": 0.678, \"ROUGE_L\": 0.499, \"METEOR\": 0.23, \"SPICE\": 0.161}", "results_details": null, "publication": ""}, {"leaderboard_id": 5, "date": "2015-08-21", "description": "Add a gated function for image features on deep RNN.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 62, "members": "", "name": "Shijian Tang"}, "results": "{\"CIDEr\": 0.671, \"Bleu_4\": 0.229, \"Bleu_3\": 0.333, \"Bleu_2\": 0.48, \"Bleu_1\": 0.662, \"ROUGE_L\": 0.482, \"METEOR\": 0.207, \"SPICE\": 0.139}", "results_details": null, "publication": ""}, {"leaderboard_id": 5, "date": "2015-09-24", "description": "An alternate of long Short term memory, where the memory is semantically overlooked.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 63, "members": "", "name": "GabYesh"}, "results": "{\"CIDEr\": 0.428, \"Bleu_4\": 0.158, \"Bleu_3\": 0.234, \"Bleu_2\": 0.36, \"Bleu_1\": 0.555, \"ROUGE_L\": 0.408, \"METEOR\": 0.162, \"SPICE\": 0.092}", "results_details": null, "publication": ""}, {"leaderboard_id": 5, "date": "2015-04-15", "description": "One layer Vanilla LSTM with 512 hidden units, 512-D words vectors trained from scratch. Beam size 1. Trained with RMSProp, gradient clipping.", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1412.2306", "team": {"id": 52, "members": "", "name": "NeuralTalk"}, "results": "{\"CIDEr\": 0.674, \"Bleu_4\": 0.224, \"Bleu_3\": 0.321, \"Bleu_2\": 0.464, \"Bleu_1\": 0.65, \"ROUGE_L\": 0.475, \"METEOR\": 0.21, \"SPICE\": 0.14}", "results_details": null, "publication": "Andrej Karpathy, Li Fei-Fei, Deep Visual-Semantic Alignments for Generating Image Descriptions, CVPR 2015"}, {"leaderboard_id": 5, "date": "2015-05-29", "description": "disclosed in future publication", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1506.03995 ", "team": {"id": 53, "members": "", "name": "Brno University"}, "results": "{\"CIDEr\": 0.517, \"Bleu_4\": 0.134, \"Bleu_3\": 0.213, \"Bleu_2\": 0.339, \"Bleu_1\": 0.535, \"ROUGE_L\": 0.403, \"METEOR\": 0.195, \"SPICE\": 0.139}", "results_details": null, "publication": "Martin Kolar, Michal Hradis, Pavel Zemcik, Technical Report: Image Captioning with Semantically Similar Images, arXiv 2015"}, {"leaderboard_id": 5, "date": "2015-05-26", "description": "This is an updated version of the m-RNN model. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. We only use MS COCO data. More details can be found at http://www.stat.ucla.edu/~junhua.mao/m-RNN.html", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1412.6632", "team": {"id": 54, "members": "", "name": "m-RNN (Baidu/ UCLA)"}, "results": "{\"CIDEr\": 0.886, \"Bleu_4\": 0.302, \"Bleu_3\": 0.41, \"Bleu_2\": 0.553, \"Bleu_1\": 0.72, \"ROUGE_L\": 0.524, \"METEOR\": 0.238, \"SPICE\": 0.171}", "results_details": null, "publication": "Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan, Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN), arXiv 2014"}, {"leaderboard_id": 5, "date": "2016-03-03", "description": "In order to loss of image information and overfitting, we proposed a novel modified LSTM to handle input image at every time step.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 264, "members": "", "name": "nlab-utokyo"}, "results": "{\"CIDEr\": 0.833, \"Bleu_4\": 0.268, \"Bleu_3\": 0.371, \"Bleu_2\": 0.512, \"Bleu_1\": 0.692, \"ROUGE_L\": 0.505, \"METEOR\": 0.236, \"SPICE\": 0.168}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-03-29", "description": "Use Deeper n/w with residual connections for the language model", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 267, "members": "", "name": "AugmentCNNwithDet"}, "results": "{\"CIDEr\": 0.956, \"Bleu_4\": 0.315, \"Bleu_3\": 0.416, \"Bleu_2\": 0.553, \"Bleu_1\": 0.721, \"ROUGE_L\": 0.531, \"METEOR\": 0.251, \"SPICE\": 0.182}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-10-25", "description": "Image captioning by exploiting image attributes. ", "leaderboard_name": "cap-c5", "url": "https://arxiv.org/abs/1611.01646", "team": {"id": 269, "members": "", "name": "MSM@MSRA"}, "results": "{\"CIDEr\": 1.049, \"Bleu_4\": 0.343, \"Bleu_3\": 0.449, \"Bleu_2\": 0.588, \"Bleu_1\": 0.751, \"ROUGE_L\": 0.552, \"METEOR\": 0.266, \"SPICE\": 0.197}", "results_details": "", "publication": "Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, Tao Mei, Boosting Image Captioning with Attributes, arXiv 2016"}, {"leaderboard_id": 5, "date": "2016-05-25", "description": "an LSTM autoencoder with a recurrent spatial transformer is used to encode CNN features and obtain a representation, and then this representation is decoded into a language description.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 270, "members": "", "name": "zhaofang_lv"}, "results": "{\"CIDEr\": 0.906, \"Bleu_4\": 0.318, \"Bleu_3\": 0.423, \"Bleu_2\": 0.559, \"Bleu_1\": 0.719, \"ROUGE_L\": 0.531, \"METEOR\": 0.245, \"SPICE\": 0.175}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-03-13", "description": "Sequential Image Captioner, ascending order", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 271, "members": "", "name": "MSRA-MSM"}, "results": "{\"CIDEr\": 0.933, \"Bleu_4\": 0.311, \"Bleu_3\": 0.421, \"Bleu_2\": 0.567, \"Bleu_1\": 0.736, \"ROUGE_L\": 0.532, \"METEOR\": 0.246, \"SPICE\": 0.179}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-06-08", "description": "Weight the words and use the weights when training", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 272, "members": "", "name": "THU_MIG"}, "results": "{\"CIDEr\": 0.99, \"Bleu_4\": 0.323, \"Bleu_3\": 0.442, \"Bleu_2\": 0.593, \"Bleu_1\": 0.762, \"ROUGE_L\": 0.542, \"METEOR\": 0.253, \"SPICE\": 0.191}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-05-21", "description": "Resnet-based captioning", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 274, "members": "", "name": "ChalLS"}, "results": "{\"CIDEr\": 0.955, \"Bleu_4\": 0.309, \"Bleu_3\": 0.414, \"Bleu_2\": 0.553, \"Bleu_1\": 0.723, \"ROUGE_L\": 0.531, \"METEOR\": 0.252, \"SPICE\": 0.183}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-11-14", "description": "e2e-glstm-tc5", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 694, "members": "", "name": "e2e-glstm-tc5"}, "results": "{\"CIDEr\": 0.923, \"Bleu_4\": 0.3, \"Bleu_3\": 0.405, \"Bleu_2\": 0.546, \"Bleu_1\": 0.717, \"ROUGE_L\": 0.525, \"METEOR\": 0.248, \"SPICE\": 0.181}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-11-06", "description": "rnn with semantic embedding", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 695, "members": "", "name": "feng"}, "results": "{\"CIDEr\": 0.986, \"Bleu_4\": 0.323, \"Bleu_3\": 0.434, \"Bleu_2\": 0.578, \"Bleu_1\": 0.743, \"ROUGE_L\": 0.54, \"METEOR\": 0.255, \"SPICE\": 0.187}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-11-15", "description": "We use SCA-CNN model trained only in train2014 dataset", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 697, "members": "", "name": "Next"}, "results": "{\"CIDEr\": 0.912, \"Bleu_4\": 0.302, \"Bleu_3\": 0.404, \"Bleu_2\": 0.542, \"Bleu_1\": 0.712, \"ROUGE_L\": 0.524, \"METEOR\": 0.244, \"SPICE\": 0.174}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-12-03", "description": "attentive model with semantic features and external corpus", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 698, "members": "", "name": "ATT_VC_REG"}, "results": "{\"CIDEr\": 0.964, \"Bleu_4\": 0.317, \"Bleu_3\": 0.423, \"Bleu_2\": 0.563, \"Bleu_1\": 0.734, \"ROUGE_L\": 0.537, \"METEOR\": 0.254, \"SPICE\": 0.182}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-07-23", "description": "sentence conditional semantic attention method, as introduced in:  http://arxiv.org/abs/1606.04621", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1606.04621", "team": {"id": 700, "members": "", "name": "UMich-COG"}, "results": "{\"CIDEr\": 0.896, \"Bleu_4\": 0.296, \"Bleu_3\": 0.399, \"Bleu_2\": 0.539, \"Bleu_1\": 0.712, \"ROUGE_L\": 0.521, \"METEOR\": 0.244, \"SPICE\": 0.174}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-07-28", "description": "ResCeption (inception-v3 + ResNet) plus Dropout-LSTM plus beam-search2", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 701, "members": "Taewan Ethan Kim", "name": "TaewanEthanKim"}, "results": "{\"CIDEr\": 0.923, \"Bleu_4\": 0.3, \"Bleu_3\": 0.403, \"Bleu_2\": 0.543, \"Bleu_1\": 0.715, \"ROUGE_L\": 0.527, \"METEOR\": 0.249, \"SPICE\": 0.177}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-06-13", "description": "sentence guided attention model using k most consensus captions", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 702, "members": "", "name": "Postech_CV"}, "results": "{\"CIDEr\": 0.987, \"Bleu_4\": 0.321, \"Bleu_3\": 0.431, \"Bleu_2\": 0.575, \"Bleu_1\": 0.743, \"ROUGE_L\": 0.539, \"METEOR\": 0.255, \"SPICE\": 0.19}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-10-24", "description": "review network", "leaderboard_name": "cap-c5", "url": "https://arxiv.org/abs/1605.07912", "team": {"id": 703, "members": "", "name": "reviewnet"}, "results": "{\"CIDEr\": 0.965, \"Bleu_4\": 0.313, \"Bleu_3\": 0.414, \"Bleu_2\": 0.55, \"Bleu_1\": 0.72, \"ROUGE_L\": 0.533, \"METEOR\": 0.256, \"SPICE\": 0.185}", "results_details": "", "publication": "Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W. Cohen, Review Networks for Caption Generation, NIPS 2016."}, {"leaderboard_id": 5, "date": "2016-11-28", "description": "Attention semantic featuers with external textual data pretraining", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 705, "members": "", "name": "Dalab_Master_Thesis"}, "results": "{\"CIDEr\": 0.96, \"Bleu_4\": 0.316, \"Bleu_3\": 0.423, \"Bleu_2\": 0.564, \"Bleu_1\": 0.735, \"ROUGE_L\": 0.537, \"METEOR\": 0.253, \"SPICE\": 0.183}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-11-22", "description": "The IC-SCA model applies a Long Short-Term Memory (LSTM) to predict the synchronous attention, receiving shared representation between vision and text. That attention is then applied to guide a guiding LSTM (gLSTM) for description generation.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 706, "members": "", "name": "AI_BUPT"}, "results": "{\"CIDEr\": 0.89, \"Bleu_4\": 0.292, \"Bleu_3\": 0.396, \"Bleu_2\": 0.537, \"Bleu_1\": 0.71, \"ROUGE_L\": 0.52, \"METEOR\": 0.243, \"SPICE\": 0.174}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-03-17", "description": "Attention models trained with reinforcement learning.", "leaderboard_name": "cap-c5", "url": "https://arxiv.org/abs/1612.00563", "team": {"id": 707, "members": "", "name": "Watson Multimodal"}, "results": "{\"CIDEr\": 1.147, \"Bleu_4\": 0.352, \"Bleu_3\": 0.47, \"Bleu_2\": 0.619, \"Bleu_1\": 0.781, \"ROUGE_L\": 0.563, \"METEOR\": 0.27, \"SPICE\": 0.207}", "results_details": "", "publication": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, Vaibhava Goel, Self-critical Sequence Training for Image Captioning, arXiv 2016"}, {"leaderboard_id": 5, "date": "2016-09-04", "description": "trnn", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 708, "members": "", "name": "DLTC@MSR"}, "results": "{\"CIDEr\": 1.003, \"Bleu_4\": 0.331, \"Bleu_3\": 0.436, \"Bleu_2\": 0.575, \"Bleu_1\": 0.74, \"ROUGE_L\": 0.543, \"METEOR\": 0.257, \"SPICE\": 0.19}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-11-22", "description": "Use Skeleton as a key", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 709, "members": "", "name": "DONOT_FAIL_AGAIN"}, "results": "{\"CIDEr\": 1.01, \"Bleu_4\": 0.32, \"Bleu_3\": 0.425, \"Bleu_2\": 0.564, \"Bleu_1\": 0.734, \"ROUGE_L\": 0.542, \"METEOR\": 0.262, \"SPICE\": 0.199}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-05-09", "description": "Generate captions with visual attention on flexible regions.", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1506.06272", "team": {"id": 710, "members": "", "name": "TsinghuaBigeye"}, "results": "{\"CIDEr\": 0.939, \"Bleu_4\": 0.314, \"Bleu_3\": 0.418, \"Bleu_2\": 0.556, \"Bleu_1\": 0.722, \"ROUGE_L\": 0.53, \"METEOR\": 0.248, \"SPICE\": 0.181}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-06-13", "description": "Image captioning by exploiting image attributes. ", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 712, "members": "", "name": "ATT-IMG (MSM@MSRA)"}, "results": "{\"CIDEr\": 1.023, \"Bleu_4\": 0.34, \"Bleu_3\": 0.449, \"Bleu_2\": 0.59, \"Bleu_1\": 0.752, \"ROUGE_L\": 0.551, \"METEOR\": 0.262, \"SPICE\": 0.193}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-12-05", "description": "Implement CMIC", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 939, "members": "", "name": "3D&amp;CV"}, "results": "{\"CIDEr\": 0.929, \"Bleu_4\": 0.3, \"Bleu_3\": 0.399, \"Bleu_2\": 0.535, \"Bleu_1\": 0.709, \"ROUGE_L\": 0.526, \"METEOR\": 0.25, \"SPICE\": 0.179}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-11-29", "description": "CNN_RNN", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 940, "members": "", "name": "Lingxiang"}, "results": "{\"CIDEr\": 0.886, \"Bleu_4\": 0.289, \"Bleu_3\": 0.392, \"Bleu_2\": 0.532, \"Bleu_1\": 0.707, \"ROUGE_L\": 0.518, \"METEOR\": 0.244, \"SPICE\": 0.175}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2016-10-30", "description": "Optimization of image description metrics using policy gradient methods.", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1612.00370", "team": {"id": 942, "members": "", "name": "G-RMI (PG-BCMR)"}, "results": "{\"CIDEr\": 1.013, \"Bleu_4\": 0.332, \"Bleu_3\": 0.445, \"Bleu_2\": 0.591, \"Bleu_1\": 0.754, \"ROUGE_L\": 0.55, \"METEOR\": 0.257, \"SPICE\": 0.187}", "results_details": "", "publication": "Optimization of image description metrics using policy gradient methods, Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin"}, {"leaderboard_id": 5, "date": "2017-03-17", "description": "gg", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1612.00370", "team": {"id": 945, "members": "Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin ", "name": "G-RMI(PG-SPIDEr-TAG)"}, "results": "{\"CIDEr\": 0.956, \"Bleu_4\": 0.307, \"Bleu_3\": 0.416, \"Bleu_2\": 0.56, \"Bleu_1\": 0.724, \"ROUGE_L\": 0.535, \"METEOR\": 0.243, \"SPICE\": 0.173}", "results_details": "", "publication": "Optimization of image description metrics using policy gradient methods, Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin"}, {"leaderboard_id": 5, "date": "2016-12-01", "description": "", "leaderboard_name": "cap-c5", "url": "https://arxiv.org/abs/1612.01887", "team": {"id": 948, "members": "", "name": "MetaMind/VT_GT"}, "results": "{\"CIDEr\": 1.042, \"Bleu_4\": 0.336, \"Bleu_3\": 0.444, \"Bleu_2\": 0.584, \"Bleu_1\": 0.748, \"ROUGE_L\": 0.55, \"METEOR\": 0.264, \"SPICE\": 0.197}", "results_details": "", "publication": "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning, J. Lu*, C. Xiong*, D. Parikh and R. Socher"}, {"leaderboard_id": 5, "date": "2017-06-07", "description": "HEDN for image caption", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1244, "members": "", "name": "ucas_yu"}, "results": "{\"CIDEr\": 1.02, \"Bleu_4\": 0.335, \"Bleu_3\": 0.44, \"Bleu_2\": 0.578, \"Bleu_1\": 0.743, \"ROUGE_L\": 0.547, \"METEOR\": 0.264, \"SPICE\": 0.196}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2015-08-02", "description": "Add an intermediate attributes layer between CNN and LSTM", "leaderboard_name": "cap-c5", "url": "http://arxiv.org/abs/1506.01144", "team": {"id": 1246, "members": "Qi Wu\r", "name": "ACVT"}, "results": "{\"CIDEr\": 0.911, \"Bleu_4\": 0.306, \"Bleu_3\": 0.414, \"Bleu_2\": 0.556, \"Bleu_1\": 0.725, \"ROUGE_L\": 0.528, \"METEOR\": 0.246, \"SPICE\": 0.178}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-06-27", "description": "Actor-Critic", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1247, "members": "", "name": "QMUL-VISION"}, "results": "{\"CIDEr\": 1.102, \"Bleu_4\": 0.337, \"Bleu_3\": 0.459, \"Bleu_2\": 0.612, \"Bleu_1\": 0.778, \"ROUGE_L\": 0.554, \"METEOR\": 0.264, \"SPICE\": 0.203}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-02-11", "description": "Multimodal Attentive Translator", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1248, "members": "", "name": "LC-JHU"}, "results": "{\"CIDEr\": 1.029, \"Bleu_4\": 0.32, \"Bleu_3\": 0.427, \"Bleu_2\": 0.568, \"Bleu_1\": 0.734, \"ROUGE_L\": 0.54, \"METEOR\": 0.258, \"SPICE\": 0.19}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-07-22", "description": "Bottom-Up and Top-Down Attention", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1249, "members": "", "name": "panderson@MSR/ACRV"}, "results": "{\"CIDEr\": 1.179, \"Bleu_4\": 0.369, \"Bleu_3\": 0.491, \"Bleu_2\": 0.641, \"Bleu_1\": 0.802, \"ROUGE_L\": 0.571, \"METEOR\": 0.276, \"SPICE\": 0.215}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-07-22", "description": "CASIA_IVA_DA", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1250, "members": "", "name": "CASIA_IVA"}, "results": "{\"CIDEr\": 1.17, \"Bleu_4\": 0.368, \"Bleu_3\": 0.484, \"Bleu_2\": 0.629, \"Bleu_1\": 0.786, \"ROUGE_L\": 0.572, \"METEOR\": 0.274, \"SPICE\": 0.213}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-06-11", "description": "Encode image features with CNN and then decode them with an LSTM network.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1251, "members": "", "name": "DeepDzayer"}, "results": "{\"CIDEr\": 0.715, \"Bleu_4\": 0.227, \"Bleu_3\": 0.33, \"Bleu_2\": 0.479, \"Bleu_1\": 0.666, \"ROUGE_L\": 0.48, \"METEOR\": 0.215, \"SPICE\": 0.149}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-08-07", "description": "multi-attention and RL.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1253, "members": "", "name": "TencentVision"}, "results": "{\"CIDEr\": 1.196, \"Bleu_4\": 0.363, \"Bleu_3\": 0.485, \"Bleu_2\": 0.635, \"Bleu_1\": 0.795, \"ROUGE_L\": 0.573, \"METEOR\": 0.277, \"SPICE\": 0.213}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-01-21", "description": "mil_vec+2-lstm", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1254, "members": "", "name": "chapternew"}, "results": "{\"CIDEr\": 0.868, \"Bleu_4\": 0.274, \"Bleu_3\": 0.378, \"Bleu_2\": 0.522, \"Bleu_1\": 0.697, \"ROUGE_L\": 0.515, \"METEOR\": 0.244, \"SPICE\": 0.174}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-03-15", "description": "Unified attributes network for image captioning ", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1255, "members": "", "name": "HUST_2017"}, "results": "{\"CIDEr\": 0.985, \"Bleu_4\": 0.324, \"Bleu_3\": 0.431, \"Bleu_2\": 0.571, \"Bleu_1\": 0.737, \"ROUGE_L\": 0.54, \"METEOR\": 0.256, \"SPICE\": 0.189}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-07-22", "description": "dascup", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1256, "members": "", "name": "DEEPAI"}, "results": "{\"CIDEr\": 1.173, \"Bleu_4\": 0.368, \"Bleu_3\": 0.485, \"Bleu_2\": 0.629, \"Bleu_1\": 0.786, \"ROUGE_L\": 0.572, \"METEOR\": 0.275, \"SPICE\": 0.213}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-03-28", "description": "mil+att+lstm", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1258, "members": "", "name": "zhangjie"}, "results": "{\"CIDEr\": 0.871, \"Bleu_4\": 0.277, \"Bleu_3\": 0.382, \"Bleu_2\": 0.526, \"Bleu_1\": 0.7, \"ROUGE_L\": 0.517, \"METEOR\": 0.244, \"SPICE\": 0.174}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-05-31", "description": "RNN+LCNN (test)", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1259, "members": "", "name": "rose"}, "results": "{\"CIDEr\": 0.915, \"Bleu_4\": 0.3, \"Bleu_3\": 0.406, \"Bleu_2\": 0.548, \"Bleu_1\": 0.721, \"ROUGE_L\": 0.526, \"METEOR\": 0.246, \"SPICE\": 0.178}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-03-24", "description": "Caption model", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1260, "members": "", "name": "kboris_plapko_viktor"}, "results": "{\"CIDEr\": 0.46, \"Bleu_4\": 0.169, \"Bleu_3\": 0.256, \"Bleu_2\": 0.394, \"Bleu_1\": 0.591, \"ROUGE_L\": 0.429, \"METEOR\": 0.174, \"SPICE\": 0.103}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-03-15", "description": "mixer-adam-bcmr", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1261, "members": "", "name": "G"}, "results": "{\"CIDEr\": 0.991, \"Bleu_4\": 0.317, \"Bleu_3\": 0.431, \"Bleu_2\": 0.579, \"Bleu_1\": 0.747, \"ROUGE_L\": 0.545, \"METEOR\": 0.258, \"SPICE\": 0.191}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-04-29", "description": "attentive linear transformation", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1262, "members": "", "name": "SenmaoYe"}, "results": "{\"CIDEr\": 1.053, \"Bleu_4\": 0.341, \"Bleu_3\": 0.443, \"Bleu_2\": 0.577, \"Bleu_1\": 0.742, \"ROUGE_L\": 0.552, \"METEOR\": 0.27, \"SPICE\": 0.2}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-07-17", "description": "GOOG+LSTM", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1263, "members": "", "name": "dreamteam"}, "results": "{\"CIDEr\": 0.809, \"Bleu_4\": 0.272, \"Bleu_3\": 0.372, \"Bleu_2\": 0.511, \"Bleu_1\": 0.687, \"ROUGE_L\": 0.504, \"METEOR\": 0.229, \"SPICE\": 0.158}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-03-30", "description": "USING ATTRIBUTE AND ATTENTION&amp;GUIDED", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1264, "members": "", "name": "SCU-IMAGE"}, "results": "{\"CIDEr\": 0.897, \"Bleu_4\": 0.295, \"Bleu_3\": 0.393, \"Bleu_2\": 0.529, \"Bleu_1\": 0.704, \"ROUGE_L\": 0.518, \"METEOR\": 0.245, \"SPICE\": 0.174}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-01-12", "description": "Image captioning with semantic attributes", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1265, "members": "", "name": "SRCB@Ricoh"}, "results": "{\"CIDEr\": 1.046, \"Bleu_4\": 0.344, \"Bleu_3\": 0.449, \"Bleu_2\": 0.586, \"Bleu_1\": 0.749, \"ROUGE_L\": 0.552, \"METEOR\": 0.266, \"SPICE\": 0.196}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-03-27", "description": "methods based on RNN and learning to form guiding information", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1266, "members": "", "name": "TencentVision"}, "results": "{\"CIDEr\": 1.028, \"Bleu_4\": 0.34, \"Bleu_3\": 0.444, \"Bleu_2\": 0.58, \"Bleu_1\": 0.744, \"ROUGE_L\": 0.548, \"METEOR\": 0.264, \"SPICE\": 0.195}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-05-08", "description": "tags", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1267, "members": "", "name": "UPC2017"}, "results": "{\"CIDEr\": 1.007, \"Bleu_4\": 0.324, \"Bleu_3\": 0.431, \"Bleu_2\": 0.571, \"Bleu_1\": 0.738, \"ROUGE_L\": 0.543, \"METEOR\": 0.259, \"SPICE\": 0.192}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-06-13", "description": "CNN with LSTM", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1268, "members": "", "name": "Lingxiang_UTS"}, "results": "{\"CIDEr\": 0.956, \"Bleu_4\": 0.303, \"Bleu_3\": 0.408, \"Bleu_2\": 0.548, \"Bleu_1\": 0.719, \"ROUGE_L\": 0.53, \"METEOR\": 0.255, \"SPICE\": 0.188}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-08-02", "description": "first detect words, then generate caption", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1269, "members": "", "name": "bmc-uestc"}, "results": "{\"CIDEr\": 1.046, \"Bleu_4\": 0.342, \"Bleu_3\": 0.447, \"Bleu_2\": 0.584, \"Bleu_1\": 0.746, \"ROUGE_L\": 0.553, \"METEOR\": 0.268, \"SPICE\": 0.199}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-05-31", "description": "Resnet101+Language CNN+RNN", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1270, "members": "", "name": "NTU_ROSE_CAP"}, "results": "{\"CIDEr\": 0.992, \"Bleu_4\": 0.31, \"Bleu_3\": 0.429, \"Bleu_2\": 0.583, \"Bleu_1\": 0.766, \"ROUGE_L\": 0.542, \"METEOR\": 0.256, \"SPICE\": 0.193}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-02-24", "description": "an improvement of im2txt", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1271, "members": "", "name": "Kyle Yang"}, "results": "{\"CIDEr\": 0.934, \"Bleu_4\": 0.306, \"Bleu_3\": 0.405, \"Bleu_2\": 0.541, \"Bleu_1\": 0.713, \"ROUGE_L\": 0.529, \"METEOR\": 0.251, \"SPICE\": 0.177}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-05-29", "description": "The method is based on a new framework which combines objects attention with attributes", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1272, "members": "", "name": "licongthu"}, "results": "{\"CIDEr\": 0.96, \"Bleu_4\": 0.314, \"Bleu_3\": 0.419, \"Bleu_2\": 0.56, \"Bleu_1\": 0.73, \"ROUGE_L\": 0.532, \"METEOR\": 0.252, \"SPICE\": 0.185}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-02-02", "description": "hitachi", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1273, "members": "", "name": "hitachi"}, "results": "{\"CIDEr\": 0.81, \"Bleu_4\": 0.251, \"Bleu_3\": 0.356, \"Bleu_2\": 0.503, \"Bleu_1\": 0.685, \"ROUGE_L\": 0.497, \"METEOR\": 0.231, \"SPICE\": 0.163}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-06-13", "description": "We firstly detect words form image, then generate captions using detected words.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1274, "members": "", "name": "CAP_BMC"}, "results": "{\"CIDEr\": 1.042, \"Bleu_4\": 0.34, \"Bleu_3\": 0.444, \"Bleu_2\": 0.581, \"Bleu_1\": 0.744, \"ROUGE_L\": 0.551, \"METEOR\": 0.268, \"SPICE\": 0.199}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-07-31", "description": "composed GAN and cider", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1275, "members": "", "name": "DCAIL04"}, "results": "{\"CIDEr\": 0.952, \"Bleu_4\": 0.301, \"Bleu_3\": 0.414, \"Bleu_2\": 0.565, \"Bleu_1\": 0.739, \"ROUGE_L\": 0.525, \"METEOR\": 0.239, \"SPICE\": 0.168}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-08-02", "description": "image caption with Part of Speech Guided", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1287, "members": "", "name": "bai&amp;amp;#39;s la"}, "results": "{\"CIDEr\": 0.942, \"Bleu_4\": 0.313, \"Bleu_3\": 0.42, \"Bleu_2\": 0.56, \"Bleu_1\": 0.728, \"ROUGE_L\": 0.532, \"METEOR\": 0.249, \"SPICE\": 0.182}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2015-10-29", "description": "", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1289, "members": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo", "name": ""}, "results": "{\"CIDEr\": 0.943, \"Bleu_4\": 0.316, \"Bleu_3\": 0.424, \"Bleu_2\": 0.565, \"Bleu_1\": 0.731, \"ROUGE_L\": 0.535, \"METEOR\": 0.25, \"SPICE\": 0.182}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-02-09", "description": "Implement CMIC", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1290, "members": "", "name": "3D&amp;amp;CV"}, "results": "{\"CIDEr\": 0.948, \"Bleu_4\": 0.305, \"Bleu_3\": 0.405, \"Bleu_2\": 0.541, \"Bleu_1\": 0.714, \"ROUGE_L\": 0.53, \"METEOR\": 0.253, \"SPICE\": 0.181}", "results_details": "", "publication": ""}, {"leaderboard_id": 5, "date": "2017-07-29", "description": "A structured semantic embedding method for image captioning, which employs visual semantic parsing tree and structured semantic attention.", "leaderboard_name": "cap-c5", "url": "", "team": {"id": 1292, "members": "Media Analysis and Computing Lab, Xiamen university\r", "name": "XMU&amp;TencentYouTu"}, "results": "{\"CIDEr\": 0.953, \"Bleu_4\": 0.32, \"Bleu_3\": 0.423, \"Bleu_2\": 0.558, \"Bleu_1\": 0.724, \"ROUGE_L\": 0.534, \"METEOR\": 0.251, \"SPICE\": 0.183}", "results_details": "", "publication": ""}]
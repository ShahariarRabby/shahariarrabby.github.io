[{"description":"We take the top-down pipeline by first detecting the person by our MegDet [1] system. The performance of person detection is 55.4 on COCO validation set (mmAP on 80 classes).   We adopt an iterative approach to train the pose estimator. In the first stage, all the data are used for the training. In the latter stage, we sample more difficult data (with low OKS) to refine the estimator. We iterate the training for four times.   Using a ResNet50 as the backbone, our single model can achieve 77.1 on test-dev. An ensemble of four model (using res50 backbone) achieved 78.1 on test-dev and 76.4 on test-challenge. External data was used in our final submission with 0.7 mAP gain on the validation set.  Each model was trained on a single machine with 8 GTX 1080Ti GPU cards.  [1] MegDet: A Large Mini-Batch Object Detector, Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, Jian Sun, CVPR 2018","url":"","results":"{\"AP_50\": \"0.929\", \"AP\": \"0.764\", \"AP_medium\": \"0.714\", \"AR\": \"0.822\", \"AR_50\": \"0.960\", \"AP_large\": \"0.832\", \"AR_large\": \"0.886\", \"AR_75\": \"0.877\", \"AR_medium\": \"0.775\", \"AP_75\": \"0.826\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"Megvii (Face++)","members":"Zhicheng Wang, Wenbo Li,  Binyi Yin, Qixiang Peng, Tianzi Xiao, Yuming Du, Zeming Li, Xiangyu Zhang, Gang Yu,  Jian Sun; Megvii (Face++)"},"date":"2018-09-09","publication":""},{"description":"Our method is based on our ECCV2018 paper(https://arxiv.org/abs/1804.06208) and our triangle-net which is a multi-branch network for human pose estimation. The final results are got by using extra data(https://challenger.ai/competition/keypoint/subject) for training and models ensemble for test(5 models based on our approach of our ECCV2018 paper and 1 model based our new triangle-net for pose estimation). Our best single model has 0.754 of mAP on COCO test-dev dataset, and ensemble model has 0.765 of mAP on COCO test-dev dataset and 0.745 of mAP on COCO test-std dataset.","url":"https://arxiv.org/abs/1804.06208","results":"{\"AP_50\": \"0.909\", \"AP\": \"0.745\", \"AP_medium\": \"0.695\", \"AR\": \"0.805\", \"AR_50\": \"0.951\", \"AP_large\": \"0.829\", \"AR_large\": \"0.875\", \"AR_75\": \"0.863\", \"AR_medium\": \"0.753\", \"AP_75\": \"0.808\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"MSRA","members":"Bin Xiao(Microsoft Research Aisa), Dianqi Li(Microsoft Research), Ke Sun(Microsoft Research Aisa), Lei Zhang(Microsoft Research), Jingdong Wang(Microsoft Research Asia)"},"date":"2018-09-09","publication":""},{"description":"We built a two-stage system,  which estimated single person skeleton after human boxes were detetected.  We first trained a mask-rcnn similar model with only coco-2017 training set for human detection. Then we trained pose   estimator(s) to predict keypoint heatmap and offset based on detected human box.  Resnet-101, resnet-152 and resnext-152 were used as backbones and Deconv structure was utilized to enlarge final resolution. Coco-2017 trainval set and extra in-house dataset were used in training.  Without any test strategy,  our single pose-estimator got 74.2 AP in test-  dev  and  72.3 AP in test-challeng set.  The final ensemble result used four pose-estimators  with fliping and multi-scale testing strategies, and reached 75.9 AP in test-dev and 74.1 AP in test-challenge set.","url":"","results":"{\"AP_50\": \"0.906\", \"AP\": \"0.741\", \"AP_medium\": \"0.685\", \"AR\": \"0.795\", \"AR_50\": \"0.944\", \"AP_large\": \"0.821\", \"AR_large\": \"0.868\", \"AR_75\": \"0.851\", \"AR_medium\": \"0.741\", \"AP_75\": \"0.804\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"The Sea Monsters","members":"Rui Zhang*, Yinglun Liu*, Rui Wu, Xinze Chen, Guan Huang (* indicates equal contribution) "},"date":"2018-09-09","publication":""},{"description":"We implement a two stage top-down method. In the first stage, we use mask rcnn to detect human bounding boxes and the person detection AP is about 53 on the test dev set. In the second stage, we perform single person pose estimation. We train Resnet based models to predict heatmap of keypoints for the person in the bounding box. We use Bi-FPN to get multi scale feature map and structure loss to learn the human skeletal structures. Before predicting, we also add a convolutional block attention module to refine the final feature map. During training, we use additional data from AI-Challenger Human Skeletal System Keypoints Detection (2017) and it provides about 1.0 point AP improvement on the validation dataset. For single model, we obtain an AP of 75.2 on the test-dev set. Finally, we use an ensemble of 8 models to obtain an AP of 76.0 on the test-dev set.","url":"","results":"{\"AP_50\": \"0.900\", \"AP\": \"0.738\", \"AP_medium\": \"0.687\", \"AR\": \"0.795\", \"AR_50\": \"0.944\", \"AP_large\": \"0.806\", \"AR_large\": \"0.866\", \"AR_75\": \"0.850\", \"AR_medium\": \"0.743\", \"AP_75\": \"0.798\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"DGDBQ","members":"Kunlin Yang (Tsinghua University) Maoqing Tian (Beihang University) Mingyuan Zhang (Beihang University)"},"date":"2018-09-09","publication":""},{"description":"We use a two-stage method: 1) Faster RCNN + SoftNMS (mAP 47% on COCO test-dev) to detect human boxes; 2) Cascade Pyramid Network with Deconvolution Head to detect human keypoints. We first train models using COCO dataset combined with AI-Challenger dataset, and then finetune on COCO dataset, which brings 1% improvement compared with training on COCO dataset only. We train different versions based on different backbones including: ResNet101 (single model AP: 75% on COCO val, 73.4% on COCO test-dev), Se-ResNeXt101 (single model AP: 74.6% on COCO val), ResNet101-dilation(Large-feature, single model AP: 74.9% on COCO val). Finally we ensemble 4 models including: 1 ResNet101 model + 2 Se-ResNeXt101 models + 1 ResNet101-dilation model and obtain an AP of 75.1% on COCO test-dev and 72.8% on COCO test-challenge.","url":"","results":"{\"AP_50\": \"0.904\", \"AP\": \"0.728\", \"AP_medium\": \"0.685\", \"AR\": \"0.796\", \"AR_50\": \"0.948\", \"AP_large\": \"0.8\", \"AR_large\": \"0.863\", \"AR_75\": \"0.855\", \"AR_medium\": \"0.747\", \"AP_75\": \"0.794\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"KPLab","members":"Boyuan Sun, Hang Zhang, Zixuan Guan, Hao Zhao, Haiyong Sun, Zhaonan Wang; (Yi+ AILab from Beijing Moshanghua Tech)"},"date":"2018-09-09","publication":""},{"description":"We handle the multi-person pose estimation with the top-down pipeline. The channel shuffle module is proposed to promote the cross-channel information communications among the feature maps across all scales, and a spatial, channel-wise attention residual bottleneck is designed to adaptively highlight the fused pyramid feature maps both in the spatial and channel-wise context. Our model is only trained on the COCO train dataset, with no extra data involved. On the COCO test-dev dataset, our single model achieves 73.6 AP with ResNet-101 backbone and 73.9 AP with ResNet-152 backbone. The ensemble model with ResNet-101 and ResNet-152 achieves 74.2 AP on the test-dev dataset and 72.8 AP on the test-challenge dataset.","url":"","results":"{\"AP_50\": \"0.906\", \"AP\": \"0.728\", \"AP_medium\": \"0.685\", \"AR\": \"0.796\", \"AR_50\": \"0.947\", \"AP_large\": \"0.8\", \"AR_large\": \"0.862\", \"AR_75\": \"0.854\", \"AR_medium\": \"0.747\", \"AP_75\": \"0.794\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"ByteDance-SEU","members":"Kai Su[1,2,*], Dongdong Yu[1,*], Zhenqi Xu[1], Xin Geng[2], Changhu Wang[1]. 1: ByteDance AI Lab, 2: Southeast University, * means Equal Contributions."},"date":"2018-09-09","publication":""},{"description":"Our system is based on a top-down two stage method. Firstly, a human detector (we used Mask R-CNN) detects humans from a given input image. The detected human bounding boxes are fed to the second stage (pose estimation network). The pose estimation network is designed to be a refinement network which takes human bounding box and corresponding heatmaps for each joint and outputs refined heatmaps. We used some previous works to generate initial heatmaps. Four NVIDIA 1080ti GPUs are used for training and testing our model. No external data is used for training. We did not use model ensemble strategy.","url":"","results":"{\"AP_50\": \"0.897\", \"AP\": \"0.727\", \"AP_medium\": \"0.685\", \"AR\": \"0.788\", \"AR_50\": \"0.942\", \"AP_large\": \"0.799\", \"AR_large\": \"0.851\", \"AR_75\": \"0.845\", \"AR_medium\": \"0.742\", \"AP_75\": \"0.789\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"SNU CVLAB","members":"A list of team members: Gyeongsik Moon (Seoul National University), Ju Yong Chang (Kwangwoon University), Geonwoon Jang (Seoul National University), Kyoung Mu Lee (Seoul National University)."},"date":"2018-09-09","publication":""},{"description":"All the models we used in COCO Keypoints Detection Challenge 2018 were only trained on COCO2017 dataset. The input size is 384x288 for all the models. The best performance 73.0 mAP on test-dev2017 was achieved by the method of multi-context attention mechanism[1] utilized in two-staged CPN[2]. The CPN is based on ResNet-152. Our detection framework is the keypoint-only Mask R-CNN[3] based on the ResNeXt-101-FPN, whose human AP is 58.5 on test-dev2018 with SoftNMS. The final result in test-challenge2018 was the ensemble of the multi-context attentional CPN and the original CPN.","url":"[1]https://arxiv.org/abs/1702.07432; [2]https://arxiv.org/abs/1702.07432; [3]https://arxiv.org/abs/1703.06870;","results":"{\"AP_50\": \"0.899\", \"AP\": \"0.725\", \"AP_medium\": \"0.683\", \"AR\": \"0.797\", \"AR_50\": \"0.947\", \"AP_large\": \"0.8\", \"AR_large\": \"0.866\", \"AR_75\": \"0.854\", \"AR_medium\": \"0.746\", \"AP_75\": \"0.791\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"fadivugibs","members":"Kaiyu Yue (Baidu VIS); Jiangfan Deng (Baidu VIS); Ailing Zeng (The Chinese University of HongKong); Xiao Chu (Baidu Research); Chao Li (Baidu VIS); Yi Yang (Baidu Research); Feng Zhou (Baidu Research); Shilei Wen (Baidu VIS); Ming Sun (Baidu VIS);"},"date":"2018-09-09","publication":""},{"description":" no any other external data\uff0cthe result i submit is single model\uff0c not ensemble","url":"","results":"{\"AP_50\": \"0.893\", \"AP\": \"0.711\", \"AP_medium\": \"0.666\", \"AR\": \"0.785\", \"AR_50\": \"0.942\", \"AP_large\": \"0.789\", \"AR_large\": \"0.86\", \"AR_75\": \"0.841\", \"AR_medium\": \"0.73\", \"AP_75\": \"0.775\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"jd_y","members":" Boyan Zhou JD"},"date":"2018-09-09","publication":""},{"description":"For detection, we use four different detection models for ensemble trained on COCO 2017. For single person pose model, we use ResNet152 as backbone, following three deconvolution layers and one skip connection to predict the pose. The model trained on COCO 2017 with segmentation. Also we use structure loss and multi-scale supervision during training.","url":"","results":"{\"AP_50\": \"0.889\", \"AP\": \"0.704\", \"AP_medium\": \"0.668\", \"AR\": \"0.768\", \"AR_50\": \"0.937\", \"AP_large\": \"0.77\", \"AR_large\": \"0.833\", \"AR_75\": \"0.829\", \"AR_medium\": \"0.721\", \"AP_75\": \"0.771\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"ByteCV","members":"Hengkai Guo, Tang Tang, Guozhong Luo, Riwei Chen, Yongchen Lu, Linfu Wen; Bytedance AI Lab"},"date":"2018-09-09","publication":""},{"description":"We adopt the top-down method by first detecting human and then do the single person pose estimation. We design an efficient yet accurate single person pose estimation network. Combined with our previous proposed RMPE framework (https://arxiv.org/abs/1612.00137), we are able to achieve 70+ AP without ensembling and can run on COCO test set at 20 fps. Code is publicly available at  https://github.com/MVIG-SJTU/AlphaPose","url":"https://github.com/MVIG-SJTU/AlphaPose","results":"{\"AP_50\": \"0.879\", \"AP\": \"0.702\", \"AP_medium\": \"0.654\", \"AR\": \"0.766\", \"AR_50\": \"0.923\", \"AP_large\": \"0.77\", \"AR_large\": \"0.835\", \"AR_75\": \"0.82\", \"AR_medium\": \"0.714\", \"AP_75\": \"0.765\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"Fast-20-FPS","members":"*Jiefeng Li, *Hao-Shu Fang, Cewu Lu (* denotes equal co-first authorship);  Shanghai Jiao Tong University"},"date":"2018-09-09","publication":""},{"description":"MultiPoseNet, a novel bottom-up multi-person pose   estimation architecture that combines a multi-task model with a novel   assignment method. MultiPoseNet can jointly handle person detection,   keypoint detection, person segmentation and pose estimation problems.   The novel assignment method is implemented by the Pose Residual   Network (PRN) which receives keypoint and person detections, and   produces accurate poses by assigning keypoints to person instances.   Final challenge results are obtained by applying top-down refinement   over MultiPoseNet results. We use only COCO keypoints dataset to train   our models.","url":"https://arxiv.org/abs/1807.04067","results":"{\"AP_50\": \"0.869\", \"AP\": \"0.697\", \"AP_medium\": \"0.648\", \"AR\": \"0.748\", \"AR_50\": \"0.907\", \"AP_large\": \"0.772\", \"AR_large\": \"0.817\", \"AR_75\": \"0.802\", \"AR_medium\": \"0.697\", \"AP_75\": \"0.76\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"METU","members":"Muhammed Kocabas*, Salih Karagoz*, Emre Akbas*. *:   Middle East Technical University"},"date":"2018-09-09","publication":""},{"description":"We train our model based on four stacked hourglass on COCO and MPII dataset, in which the targets range from coarse to fine are used for supervisions. After that, we remove other supervised information and finetune our model using only the fine target. By doing this, we get 73.1% on val2017 dataset and 69.8% on dev2017 dataset. Our final model is the ensemble of our model with Mask-RCNN [1], which helps us achieve 74.8% on the val2017 data, 71.3% on dev2017 data. Pose NMS [2] also play an important role in finding the most confident pose, merge redundant poses and redundant poses. These are helpful in improving performance on pose estimation results.     [1] He, Kaiming, et al. \"Mask r-cnn.\" Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017.  [2] Fang, Haoshu, et al. \"Rmpe: Regional multi-person pose estimation.\" The IEEE International Conference on Computer Vision (ICCV). Vol. 2. 2017.","url":"","results":"{\"AP_50\": \"0.888\", \"AP\": \"0.691\", \"AP_medium\": \"0.646\", \"AR\": \"0.75\", \"AR_50\": \"0.929\", \"AP_large\": \"0.758\", \"AR_large\": \"0.818\", \"AR_75\": \"0.81\", \"AR_medium\": \"0.7\", \"AP_75\": \"0.758\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"Raven-DL","members":"Lianbo Zhang, University of Technology Sydney Jue Wang, University of Technology Sydney"},"date":"2018-09-09","publication":""},{"description":"We develop a novel bottom-up pose estimation method, based on an improved version of stacked hourglass network with an associative embedding loss. Our model can accomplish keypoint localization and grouping at the same time. We train our model without any extra data. Our single model achieves 0.684 AP on test-challenge and 0.702 AP on test-dev with multi-scale testing.  No model ensemble is used.","url":"","results":"{\"AP_50\": \"0.876\", \"AP\": \"0.684\", \"AP_medium\": \"0.632\", \"AR\": \"0.738\", \"AR_50\": \"0.907\", \"AP_large\": \"0.758\", \"AR_large\": \"0.82\", \"AR_75\": \"0.793\", \"AR_medium\": \"0.678\", \"AP_75\": \"0.748\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"TFMAN","members":"Xinyu Gong, Weihang Chen"},"date":"2018-09-09","publication":""},{"description":"Method description: we use resnetv2 as the backbone convolotional network to provide four pyramid group of features (outputs of endpoints 'block1','block2','block3','block4') , we regress keypoints heatmaps from each group of features independently by connecting further sequence of convolutional and upsample operations. The four sets of heatmaps resulting from differnt pyramid features are finally added together to serve as the final result. we only use data in person_keypoints_train2017, the single model best performance on test-dev is 0.65, no emsemble trick of multi models are used.","url":"","results":"{\"AP_50\": \"0.865\", \"AP\": \"0.648\", \"AP_medium\": \"0.601\", \"AR\": \"0.699\", \"AR_50\": \"0.897\", \"AP_large\": \"0.713\", \"AR_large\": \"0.766\", \"AR_75\": \"0.755\", \"AR_medium\": \"0.65\", \"AP_75\": \"0.708\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"Huya","members":"Li Baishun"},"date":"2018-09-09","publication":""},{"description":"Human pose estimation is a fundamental research topic in computer vision. This topic has been largely improved recently thanks to the development of convolution neural network. This paper proposes a new CNN architecture which combines the key-points estimators and an object detector. This network can detect poses of all people and the around objects in the image in parallel. In general, to address the multi-person pose estimation, the network generates human key-point heatmaps and bounding boxes simultaneously. Then, for each person-bounding-box, it crops image and the heatmaps based on the coordinate. These cropped images are passed through another single-person pose estimator to generate the pose of person inside the bounding box. The proposed network is just trained on the COCO train dataset with augmentation.  The proposed network is implemented based on mxnet, run on a desktop with GTX 1080Ti devices. It takes 0.244 seconds for generating heatmaps and bounding boxes, and 0.1 seconds for estimating pose of single person.","url":"","results":"{\"AP_50\": \"0.762\", \"AP\": \"0.522\", \"AP_medium\": \"0.538\", \"AR\": \"0.589\", \"AR_50\": \"0.801\", \"AP_large\": \"0.578\", \"AR_large\": \"0.64\", \"AR_75\": \"0.633\", \"AR_medium\": \"0.551\", \"AP_75\": \"0.567\"}","leaderboard_name":"kpt-challenge2018","team":{"name":"UOU_ISLab","members":"Van-Thanh Hoang, Intelligent System Lab., School of Electrical Engineering, University of Ulsan, Ulsan, Korea, Kang-Hyun Jo, Intelligent System Lab., School of Electrical Engineering, University of Ulsan, Ulsan, Korea"},"date":"2018-09-09","publication":""}]